<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Undergrads | computational human-robot interaction lab</title>
    <link>https://CHRI-Lab.github.io/prospective/undergrad/</link>
      <atom:link href="https://CHRI-Lab.github.io/prospective/undergrad/index.xml" rel="self" type="application/rss+xml" />
    <description>Undergrads</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Research Group in HRI in Australia (UoM, UNSW)</copyright>
    <image>
      <url>https://CHRI-Lab.github.io/images/icon_hu02c0217b7eeb0e7fdb8b09af6bff09b6_8240_512x512_fill_lanczos_center_2.png</url>
      <title>Undergrads</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/</link>
    </image>
    
    <item>
      <title>E-Pen</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/e-pen/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/e-pen/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Handwriting is one of the most important motor skill we learn as human. Children with handwriting difficulties can find their academic impacted and often succeed less at school. It is even more true for people having to learn different handwriting scripts (i.e. latin, chinese, arabic)&lt;/p&gt;
&lt;p&gt;In this project we propose to explore new methods to assess and train people&amp;rsquo;s handwriting in a multiscript handwriting application. The project will aim to develop a new engaging handwriting analysis tool and integrate the analysis in agamified application.
The backend of the application will be performing the analsysi of handwirting through a library taking into account various features of the handwriting logs (i.e. pen pressure, tilt, speed).&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;During this project, the student will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Develop a library able to analyse strokes and handwriting (backend)&lt;/li&gt;
&lt;li&gt;Develop a JS app able to record handwriting data&lt;/li&gt;
&lt;li&gt;Integrate gamification into the app to build a learning game&lt;/li&gt;
&lt;li&gt;Evaluate the implemented application with end-users evaluating both usability and performances (learning outcomes)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Handwriting, JS, Algoritms,&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: JS, Python, Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;See Collection &lt;a href=&#34;https://wafa.johal.org/tags/handwriting/&#34;&gt;https://wafa.johal.org/tags/handwriting/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Find Your Voice: Use of Voice Assistant for Learning</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/fyv_2020/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/fyv_2020/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://CHRI-Lab.github.io/img/student-projects/fyv2020.png&#34; alt=&#34;FYV&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Many children struggle to find their voice in social situations. They may be shy, suffer from social anxiety, be new to a culture (migrants) or have impairments in communication due to atypical development  (e.g., autism spectrum disorder, speech or hearing disorders).&lt;/p&gt;
&lt;p&gt;The voices of these children often go unheard, as they find it hard to contribute to a conversation.&lt;/p&gt;
&lt;p&gt;The Find your Voice (FyV, &lt;a href=&#34;http://wafa.johal.org/project/fyv/&#34;&gt;http://wafa.johal.org/project/fyv/&lt;/a&gt;) project was initiated to investigate how joke telling could help children to speak up and gain confidence. We are also interested in story telling and general conversation. Improvements in communication can have a significant impact in confidence, and  help children:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reduce stress&lt;/li&gt;
&lt;li&gt;improve self-confidence&lt;/li&gt;
&lt;li&gt;ease social interactions&lt;/li&gt;
&lt;li&gt;make friends more easily&lt;/li&gt;
&lt;li&gt;improving literacy and language&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To help children develop the ability to communicate, tell jokes or stories to their peers, we propose leveraging social robots (e.g. NAO) and voice assistants (e.g., Alexa, Olly and Google Home) to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model how to tell jokes/stories and respond to other children during conversations .&lt;/li&gt;
&lt;li&gt;Practice joke/story telling with a ‘friendly’ and ‘non-judgmental’ audience.&lt;/li&gt;
&lt;li&gt;Practice turn taking during conversation.&lt;/li&gt;
&lt;li&gt;TLearn jokes, stories and interesting facts to tell other children.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The overall goals of the project are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To enable children to improve their social communication skills by learning intonation and timing, through interacting with voice assistants&lt;/li&gt;
&lt;li&gt;To learn to how to perform in front of peers and family&lt;/li&gt;
&lt;li&gt;To make children more confident in social situations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The FyV project involves partners in London and California.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;At UNSW, our main goal will be to develop a ‘Learning by Teaching’ application using a robot or voice assistant. This application will allow the user to teach a virtual agent (robot or voice assistant) a joke/story. As the agent learns by demonstration, the user can practice and refine how the story/joke is told until the voice assistant (and the child) is able to tell the joke/story in a satisfactory way.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Design the Learning Scenario&lt;/li&gt;
&lt;li&gt;Explore TTS software for speech conversion&lt;/li&gt;
&lt;li&gt;Implement a new Alexa Skill&lt;/li&gt;
&lt;li&gt;Run a Pilot demonstrating the learning of joke/story telling features (e.g. pauses and intonations)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Voice Assistant, Machine Learning, HCI&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python or C++. Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/soobinseo/Transformer-TTS&#34;&gt;https://github.com/soobinseo/Transformer-TTS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/barronalex/Tacotron&#34;&gt;https://github.com/barronalex/Tacotron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.amazon.com/en-US/alexa/alexa-skills-kit&#34;&gt;https://developer.amazon.com/en-US/alexa/alexa-skills-kit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Human Action Recognition from AD Movies</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/human_action_recognition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/human_action_recognition/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Action Recognition is curcial for robots to perfoma around humans. 
Indeed, robot need to asses human action and intentions in order to assist them in everyday life tasks and to collaborative efficiently.&lt;/p&gt;
&lt;p&gt;The field of action recognition has  aimed to use typical sensors found on robots to recognize agnts, objects and actions they are performing.
Typical approach is to record a dataset of various action and label them. But often theses actions are not natural and it can be difficult to represent the variety of ways to perform actions with a lab built dataset. 
In this project we propose to use audio desription movies to label actions.
AD movies integrate a form of narration to allow virually impaired veiwers to undertsnad the visual element sowed on screen.
This information often deals with action actually depicted on the scene.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;During this project, the student will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Develop a pipeline to collect and crop clip of AD movies for at home actions. 
This extraction tool should be fexible and allow for integration of next action. It will for instance feature video and text processing to extract [Subject+ Action + Object] type of data.&lt;/li&gt;
&lt;li&gt;Investigate methods for HAR&lt;/li&gt;
&lt;li&gt;Implement a tree model combaning HAR with YOLO to identify agent and objects&lt;/li&gt;
&lt;li&gt;Evaluate the HAR pipeline with the Toyota Robot HSR&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Human Action Recognition,&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, C++, Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www-sciencedirect-com.wwwproxy1.library.unsw.edu.au/science/article/pii/S174228761930283X&#34;&gt;https://www-sciencedirect-com.wwwproxy1.library.unsw.edu.au/science/article/pii/S174228761930283X&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2015/papers/Rohrbach_A_Dataset_for_2015_CVPR_paper.pdf&#34;&gt;https://openaccess.thecvf.com/content_cvpr_2015/papers/Rohrbach_A_Dataset_for_2015_CVPR_paper.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dl-acm-org.wwwproxy1.library.unsw.edu.au/doi/abs/10.1145/3355390?casa_token=MrZSE8hoPFYAAAAA:rcwHYdISRyLM5OApuN_2SASbwgBsswxx2EPHy9mGP8NaqIdvBj0q5LIa9_ChdyI_Lzfi4GX0PWjhD54&#34;&gt;https://dl-acm-org.wwwproxy1.library.unsw.edu.au/doi/abs/10.1145/3355390?casa_token=MrZSE8hoPFYAAAAA:rcwHYdISRyLM5OApuN_2SASbwgBsswxx2EPHy9mGP8NaqIdvBj0q5LIa9_ChdyI_Lzfi4GX0PWjhD54&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://prior.allenai.org/projects/charades&#34;&gt;https://prior.allenai.org/projects/charades&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.02696.pdf&#34;&gt;https://arxiv.org/pdf/1708.02696.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1806.11230.pdf&#34;&gt;https://arxiv.org/pdf/1806.11230.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning for Social Interaction Modelling</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/hri-dataset-mine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/hri-dataset-mine/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;The field of social human-robot interaction is growing.
Understanding how communication between humans (human-human) generalises during human-robot communication is crucial in building fluent and enjoyable interactions with social robots.
Everyday, more datasets that feature social interaction between humans and between humans and robots are made freely available online.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://d3i71xaburhd42.cloudfront.net/37c0bc28388902869d904b002fe789083b610ee1/4-Figure1-1.png&#34; alt=&#34;MHHRI&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this project we propose to take a data-driven approach to build predictive models of social interactions between humans (HH) ( and between humans and robots (HR) interaction using 3 different datasets. Relevant research questions include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Which multi-modal features can be transferable from HH to HR setups?&lt;/li&gt;
&lt;li&gt;Are there common features that discriminate human behaviour in HH or HR scenarios (e.g. &amp;lsquo;Do people speak less or slower with robots?&amp;rsquo; &amp;hellip; )&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;During this project, the student will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore datasets (PinSoRo, MHHRI and P2PSTORY): type of data (video; audio, point cloud), available labels and annotation &amp;hellip;&lt;/li&gt;
&lt;li&gt;Extract relevant features multimodal on each dataset&lt;/li&gt;
&lt;li&gt;Evaluate predictive models for each dataset (i.e. engagement)&lt;/li&gt;
&lt;li&gt;Explore transfer learning from one dataset to another&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is also potential to use UNSW’s National Facility for Human-Robot Interaction Research to create a new dataset.&lt;/p&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Machine Learning, Human-Robot Interaction&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, ROS, Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205999&#34;&gt;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205999&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cl.cam.ac.uk/research/rainbow/projects/mhhri/&#34;&gt;https://www.cl.cam.ac.uk/research/rainbow/projects/mhhri/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.media.mit.edu/projects/p2pstory/overview/&#34;&gt;https://www.media.mit.edu/projects/p2pstory/overview/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Persuasive Robots - Exploring Behavioural Styles</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/behavioural-styles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/behavioural-styles/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Social robots are foreseen to be encountered in our everyday life, playing roles as assistant or companion (to mention a few).
Recent studies have shown the potential harmful impacts of overtrust in social robotics [1], as robots may collect sensitive information without the user’s knowledge.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://CHRI-Lab.github.io/img/student-projects/styles.png&#34; alt=&#34;graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;Behavioural styles  allow  robots to express themselves differently within the same context. Given a specific gesture, keyframe manipulation can be used in order to generate style-based variation to the gesture.
Behavioural styles have been studied in the past to improve robot&amp;rsquo;s behaviour during human-robot interaction [2].&lt;/p&gt;
&lt;p&gt;In this project, we will explore how behavioural styles can influence engagement, trust and persuasion during human-robot interaction.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implement behavioural styles for the Nao robot (voice and behaviour) and for a voice assistant (voice only)&lt;/li&gt;
&lt;li&gt;Design at least two behaviour styles based on human behaviour and personality styles&lt;/li&gt;
&lt;li&gt;Evaluate and compare these styles via experimentation&lt;/li&gt;
&lt;li&gt;Design a scenario similar to the one described in paper [3]&lt;/li&gt;
&lt;li&gt;Setup a data collection environment (posture, video and audio) in the HRI Lab facility of UNSW&lt;/li&gt;
&lt;li&gt;Select appropriate tasks and/or questionnaires to measure engagement, trust and/or persuasion&lt;/li&gt;
&lt;li&gt;Evaluate the system via an experiment with users&lt;/li&gt;
&lt;li&gt;Complete the data analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Robotics, HRI, Psychology&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, ROS and Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;[1]https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2019/10/14081257/Robots_social_impact_eng.pdf&lt;/li&gt;
&lt;li&gt;[2] &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.279&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Johal, W., Pesty, S., &amp;amp; Calvary, G. (2014, August). Towards companion robots behaving with style. In The 23rd IEEE International Symposium on Robot and Human Interactive Communication (pp. 1063-1068). IEEE.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[3] Bainbridge, W. A., Hart, J. W., Kim, E. S., &amp;amp; Scassellati, B. (2011). The benefits
of interactions with physically present robots over video-displayed agents.
International Journal of Social Robotics, 3(1), 41-52.&lt;/li&gt;
&lt;li&gt;[4] Peters, R., Broekens, J., Li, K., &amp;amp; Neerincx, M. A. (2019, July). Robot Dominance Expression Through Parameter-based Behaviour Modulation. In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents (pp. 224-226). ACM.&lt;/li&gt;
&lt;li&gt;[5] Shane Saunderson et al. It Would Make Me Happy if You Used My Guess: Comparing Robot Persuasive Strategies in Social Human–Robot Interaction, IEEE Robotics and Automation Letters (2019). DOI: 10.1109/LRA.2019.2897143&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Robot Writing</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/writing_2020/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/writing_2020/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Handwriting is one of the most important motor skill we learn as human. Children with handwriting difficulties can find their academic impacted and often succeed less at school.&lt;/p&gt;
&lt;p&gt;In this project we propose to explore new methods for a robot to write using a pen. The project will use the Fetch Robot and aim to evaluate how a robot could learn handwriting trajectories using human demonstrations.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;During this project, the student will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn about ROS&lt;/li&gt;
&lt;li&gt;Develop a ROS package that enables to control the Fetch Robot arm given a series of handwriting strokes&lt;/li&gt;
&lt;li&gt;Explore the use of different methods for the robot to learn letter writing from demonstrations&lt;/li&gt;
&lt;li&gt;Evaluate the implemented method compare to other state of the art methods&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;ROS, Learning by Demonstration, Robotics, Handwriting&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, C++,  ROS, Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;See Zotero Collection &lt;a href=&#34;https://www.zotero.org/groups/2419050/hri-unsw/collections/GQERYTFZ&#34;&gt;https://www.zotero.org/groups/2419050/hri-unsw/collections/GQERYTFZ&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tangible e-Ink Paper Interfaces for Learners</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/tip_2020/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/tip_2020/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://CHRI-Lab.github.io/img/student-projects/tip.png&#34; alt=&#34;FYV&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;While digital tools are more and more used in classrooms, teachers&#39; common practice remains to use photocopied paper documents to share and collect learning exercises from their students.
With the Tangible e-Ink Paper (TIP) system, we aim to explore the use of tangible manipulatives interacting with paper sheets as a bridge between digital and paper traces of learning.
Featuring an e-Ink display, a paper-based localisation system and a wireless connection, TIPs are envisioned to be used as a versatile tool across various curriculum activities.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Literature Review on Tangible UI (TUI) in Education&lt;/li&gt;
&lt;li&gt;Implement and test a proof of concept of TIPs for learning&lt;/li&gt;
&lt;li&gt;Assemble 3 TIPs (3D printing of parts, soldering, etc.)&lt;/li&gt;
&lt;li&gt;Install libraries on Rasberry PI ( e.g. libdots - used for self-paper-based localisation, bluetooth communication)&lt;/li&gt;
&lt;li&gt;Develop two demo applications using TIPs for
&lt;ul&gt;
&lt;li&gt;individual work (on A4 sheet of paper)&lt;/li&gt;
&lt;li&gt;collaborative work (on min A2)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Tangible User Interfaces, HCI&lt;/p&gt;
&lt;h2 id=&#34;prerequisites-and-learning-outcomes&#34;&gt;Prerequisites and Learning Outcomes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Javascript, Python or C++. Git.&lt;/li&gt;
&lt;li&gt;Qt, Rasberry Pi&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://infoscience.epfl.ch/record/271833/files/paper.pdf&#34;&gt;https://infoscience.epfl.ch/record/271833/files/paper.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://infoscience.epfl.ch/record/224129/files/paper.pdf&#34;&gt;https://infoscience.epfl.ch/record/224129/files/paper.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://infoscience.epfl.ch/record/270077/files/Robot_Analytics.pdf&#34;&gt;https://infoscience.epfl.ch/record/270077/files/Robot_Analytics.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-29736-7_57&#34;&gt;https://link.springer.com/chapter/10.1007/978-3-030-29736-7_57&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Tangible Human Swarm Interaction</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/h-swarm_2020/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/h-swarm_2020/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://CHRI-Lab.github.io/img/student-projects/swarm.png&#34; alt=&#34;graph&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context:&lt;/h2&gt;
&lt;p&gt;Visuo-Motor coordination problems can impair children in  their academic achievements and in their everyday life. 
Gross visuo-motor skills, in particular, are required in a range of social and educational activities that contribute to children&amp;rsquo;s physical and cognitive development such as playing a musical instrument, ball-based sports or dancing. 
Children with visuo-motor coordination difficulties are typically diagnosed with developmental coordination disorder  or cerebral palsy and need undergo physical therapy. 
The therapy sessions are often not engaging for children and conducted individually. 
In this project, we aim to design new forms of interaction with a swarm for enhance visuo-motor coordination. We propose to develop a game that allows multiple children to play collaboratively on the same table.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implement the set of basic swarm behaviour using 4 Cellulo robots&lt;/li&gt;
&lt;li&gt;Integrate collaorative and tangible interactions&lt;/li&gt;
&lt;li&gt;Test the system with a participants. We plan ti integrate a measure of cognitive load using eye tracking data&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;HCI, Health, Game, Swarm Robotics&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, C++, Js&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.epfl.ch/labs/chili/index-html/research/cellulo/&#34;&gt;https://www.epfl.ch/labs/chili/index-html/research/cellulo/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Tangible Robots for Collaborative Online Learning</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/tangible_online/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/tangible_online/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://CHRI-Lab.github.io/img/cellulo.png&#34; alt=&#34;graph&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context:&lt;/h2&gt;
&lt;p&gt;Online learning  presents several advantages: decreasing cost, allowing more flexibility and access to far away training resources. However, studies have found that it also limits communications between peers and teachers, limits physical interactions and that it requires a big commitment on the student&amp;rsquo;s part to plan and stay assiduous in their learning.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;In this project, we aim to design and test a novel way to engage students in collaborative online learning by using haptic enabled tangible robots.
The project will consist in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;developing a tool allowing the design of online activities for two or more robots to be connected&lt;/li&gt;
&lt;li&gt;implementing a demonstrator for this new library that will embed a series of small exercises hilightling the new capability of remote haptic-assisted collaboration&lt;/li&gt;
&lt;li&gt;evaluating the demonstrator with a user experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;HCI, Haptics, Robot, Collaborative Work (Training/Gaminig)&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: C++, Js,&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;See Zotero Collection &lt;a href=&#34;https://www.zotero.org/groups/2419050/hri-unsw/collections/JXBHFMBC&#34;&gt;https://www.zotero.org/groups/2419050/hri-unsw/collections/JXBHFMBC&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Schneider, B., Jermann, P., Zufferey, G., &amp;amp; Dillenbourg, P. (2011). Benefits of a Tangible Interface for Collaborative Learning and Interaction. IEEE Transactions on Learning Technologies, 4(3), 222–232. &lt;a href=&#34;https://doi.org/10.1109/TLT.2010.36&#34;&gt;https://doi.org/10.1109/TLT.2010.36&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Asselborn, T., Guneysu, A., Mrini, K., Yadollahi, E., Ozgur, A., Johal, W., &amp;amp; Dillenbourg, P. (2018). Bringing letters to life: Handwriting with haptic-enabled tangible robots. Proceedings of the 17th ACM Conference on Interaction Design and Children, 219–230.&lt;/li&gt;
&lt;li&gt;East, B., DeLong, S., Manshaei, R., Arif, A., &amp;amp; Mazalek, A. (2016). Actibles: Open Source Active Tangibles. Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces, 469–472. &lt;a href=&#34;https://doi.org/10.1145/2992154.2996874&#34;&gt;https://doi.org/10.1145/2992154.2996874&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guinness, D., Muehlbradt, A., Szafir, D., &amp;amp; Kane, S. K. (2019a). RoboGraphics: Dynamic Tactile Graphics Powered by Mobile Robots. The 21st International ACM SIGACCESS Conference on Computers and Accessibility, 318–328. &lt;a href=&#34;https://doi.org/10.1145/3308561.3353804&#34;&gt;https://doi.org/10.1145/3308561.3353804&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guinness, D., Muehlbradt, A., Szafir, D., &amp;amp; Kane, S. K. (2019b). RoboGraphics: Using Mobile Robots to Create Dynamic Tactile Graphics. The 21st International ACM SIGACCESS Conference on Computers and Accessibility, 673–675. &lt;a href=&#34;https://doi.org/10.1145/3308561.3354597&#34;&gt;https://doi.org/10.1145/3308561.3354597&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guinness, D., Muehlbradt, A., Szafir, D., &amp;amp; Kane, S. K. (2018). The Haptic Video Player: Using Mobile Robots to Create Tangible Video Annotations. Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces, 203–211. &lt;a href=&#34;https://doi.org/10.1145/3279778.3279805&#34;&gt;https://doi.org/10.1145/3279778.3279805&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guneysu, A., Johal, W., Ozgur, A., &amp;amp; Dillenbourg, P. (2018). Tangible Robots Mediated Collaborative Rehabilitation Design: Can we Find Inspiration from Scripting Collaborative Learning? Workshop on Robots for Learning R4L HRI2018.&lt;/li&gt;
&lt;li&gt;Guneysu Ozgur, A., Wessel, M. J., Johal, W., Sharma, K., Ozgur, A., Vuadens, P., Mondada, F., Hummel, F. C., &amp;amp; Dillenbourg, P. (2018). Iterative design of an upper limb rehabilitation game with tangible robots. Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, 241–250.&lt;/li&gt;
&lt;li&gt;Guneysu Ozgur, A., Wessel, M. J., Olsen, J. K., Johal, W., Özgür, A., Hummel, F. C., &amp;amp; Dillenbourg, P. (2020). Gamified Motor Training with Tangible Robots in Older Adults: A Feasibility Study and Comparison with Young. Frontiers in Aging Neuroscience, 12. &lt;a href=&#34;https://doi.org/10.3389/fnagi.2020.00059&#34;&gt;https://doi.org/10.3389/fnagi.2020.00059&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ishii, H., &amp;amp; Ullmer, B. (1997). Tangible Bits: Towards Seamless Interfaces Between People, Bits and Atoms. Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 234–241. &lt;a href=&#34;https://doi.org/10.1145/258549.258715&#34;&gt;https://doi.org/10.1145/258549.258715&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Johal, W., Tran, A., Khodr, H., Özgür, A., &amp;amp; Dillenbourg, P. (2019). TIP: Tangible e-Ink Paper Manipulatives for Classroom Orchestration. Proceedings of the 31st Australian Conference on Human-Computer-Interaction, 595–598. &lt;a href=&#34;https://doi.org/10.1145/3369457.3369539&#34;&gt;https://doi.org/10.1145/3369457.3369539&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Loparev, A., Westendorf, L., Flemings, M., Cho, J., Littrell, R., Scholze, A., &amp;amp; Shaer, O. (2017). BacPack: Exploring the Role of Tangibles in a Museum Exhibit for Bio-Design. Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction, 111–120. &lt;a href=&#34;https://doi.org/10.1145/3024969.3025000&#34;&gt;https://doi.org/10.1145/3024969.3025000&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Okerlund, J., Shaer, O., &amp;amp; Latulipe, C. (2016). Teaching Computational Thinking Through Bio-Design (Abstract Only). Proceedings of the 47th ACM Technical Symposium on Computing Science Education, 698. &lt;a href=&#34;https://doi.org/10.1145/2839509.2850569&#34;&gt;https://doi.org/10.1145/2839509.2850569&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;O’Malley, C., &amp;amp; Fraser, D. S. (2004). Literature review in learning with tangible technologies.&lt;/li&gt;
&lt;li&gt;Ozgur, A. G., Wessel, M. J., Asselborn, T., Olsen, J. K., Johal, W., Özgür, A., Hummel, F. C., &amp;amp; Dillenbourg, P. (2019). Designing Configurable Arm Rehabilitation Games: How Do Different Game Elements Affect User Motion Trajectories? 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 5326–5330. &lt;a href=&#34;https://doi.org/10.1109/EMBC.2019.8857508&#34;&gt;https://doi.org/10.1109/EMBC.2019.8857508&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ozgur, A., Johal, W., Mondada, F., &amp;amp; Dillenbourg, P. (2017). Haptic-enabled handheld mobile robots: Design and analysis. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 2449–2461.&lt;/li&gt;
&lt;li&gt;Ozgur, A., Lemaignan, S., Johal, W., Beltran, M., Briod, M., Pereyre, L., Mondada, F., &amp;amp; Dillenbourg, P. (2017). Cellulo: Versatile handheld robots for education. 2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI, 119–127.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Voice for ROS</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/voice-robot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/voice-robot/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Natural language is an important part of communication since it offers an intuitive and efficient way of conveying ideas to another individual. Enabling robots to efficiently use language is essential for human-robot collaboration. In this project, we aim to develop an interface between a dialog manager (i.e. DialogFlow) and ROS (Robotics Operating System). By doing this, we will be able to use the powerful dialogue systems in human-robot interaction scenario.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1066/1*OCuPx7AmWofWptQdPN-TPA.png&#34; alt=&#34;robot-va&#34;&gt;&lt;/p&gt;
&lt;p&gt;A scenario, using tangible robots (Cellulo) combined with voice assistant for upper-arm rehabilitation will be implemented to show the potential of this new ros-package.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;During this project, the student will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn about Google DialogFlow and ROS&lt;/li&gt;
&lt;li&gt;Develop a ROS package that enables to access and manipulates DialogFlow features&lt;/li&gt;
&lt;li&gt;Develop a Cellulo Rehabilitation Game&lt;/li&gt;
&lt;li&gt;Test the game with a pilot experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Voice-Assistant, Human-Robot Interaction, ROS&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, C++,  ROS, Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://dialogflow.com/&#34;&gt;https://dialogflow.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ros.org/&#34;&gt;https://www.ros.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wafa.johal.org/project/cellulo/&#34;&gt;http://wafa.johal.org/project/cellulo/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hudson, C., Bethel, C. L., Carruth, D. W., Pleva, M., Juhar, J., &amp;amp; Ondas, S. (2017, October). A training tool for speech driven human-robot interaction applications. In 2017 15th International Conference on Emerging eLearning Technologies and Applications (ICETA) (pp. 1-6). IEEE.&lt;/li&gt;
&lt;li&gt;Moore, R. K. (2017). Is spoken language all-or-nothing? Implications for future speech-based human-machine interaction. In Dialogues with Social Robots (pp. 281-291). Springer, Singapore.&lt;/li&gt;
&lt;li&gt;Beirl, D., Yuill, N., &amp;amp; Rogers, Y. (2019). Using Voice Assistant Skills in Family Life. In Lund, K., Niccolai, G. P., Lavoué, E., Gweon, C. H., &amp;amp; Baker, M. (Eds.), A Wide Lens: Combining Embodied, Enactive, Extended, and Embedded Learning in Collaborative Settings, 13th International Conference on Computer Supported Collaborative Learning (CSCL) 2019, Volume 1 (pp. 96-103). Lyon, France: International Society of the Learning Sciences.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
