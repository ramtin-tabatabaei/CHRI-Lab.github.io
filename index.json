[{"authors":["Daisy Ingle"],"categories":null,"content":"Daisy\u0026rsquo;s thesis was on\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4827f60a651d5df7ea6dba9b22ea0d72","permalink":"https://CHRI-Lab.github.io/author/daisy-ingle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/daisy-ingle/","section":"authors","summary":"Daisy\u0026rsquo;s thesis was on","tags":null,"title":"Daisy Ingle","type":"authors"},{"authors":["Duc Luong Ong"],"categories":null,"content":"Duc Luong\u0026rsquo;s thesis was on\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fb40d14078a7ed73f171f1b5d829d3b1","permalink":"https://CHRI-Lab.github.io/author/duc-luong-ong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/duc-luong-ong/","section":"authors","summary":"Duc Luong\u0026rsquo;s thesis was on","tags":null,"title":"Duc Luong Ong","type":"authors"},{"authors":["Lina Phaijit"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"42b3f0c6c6b7b3e24b91811811ccf4bb","permalink":"https://CHRI-Lab.github.io/author/lina-phaijit/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/lina-phaijit/","section":"authors","summary":"","tags":null,"title":"Lina Phaijit","type":"authors"},{"authors":["Michael O'Dea"],"categories":null,"content":"Michael O\u0026rsquo;Dea\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7c112285d9c819b96140245cc2da096b","permalink":"https://CHRI-Lab.github.io/author/michael-odea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/michael-odea/","section":"authors","summary":"Michael O\u0026rsquo;Dea","tags":null,"title":"Michael O'Dea","type":"authors"},{"authors":["Oltan Sevinc"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"daccbf67e535003c8d6dce9dae8ffff8","permalink":"https://CHRI-Lab.github.io/author/oltan-sevinc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/oltan-sevinc/","section":"authors","summary":"","tags":null,"title":"Oltan Sevinc","type":"authors"},{"authors":["Terry Agapitos"],"categories":null,"content":"Terry\u0026rsquo;s thesis was on\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a00022f80d1f30e53f815ce364cf140e","permalink":"https://CHRI-Lab.github.io/author/terry-agapitos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/terry-agapitos/","section":"authors","summary":"Terry\u0026rsquo;s thesis was on","tags":null,"title":"Terry Agapitos","type":"authors"},{"authors":null,"categories":null,"content":"Assistant Professor in the Faculty of Engineering at UNSW Sydney, Wafa obtained her PhD from the University of Grenoble (France) focusing on bodily signals in child-robot interaction and affective reasoning for social agents. Her research aims at creating acceptable and useful assistive robot interactions using social signal sensing, affective and cognitive reasoning and natural expressivity.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://CHRI-Lab.github.io/author/wafa-johal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wafa-johal/","section":"authors","summary":"Assistant Professor in the Faculty of Engineering at UNSW Sydney, Wafa obtained her PhD from the University of Grenoble (France) focusing on bodily signals in child-robot interaction and affective reasoning for social agents. Her research aims at creating acceptable and useful assistive robot interactions using social signal sensing, affective and cognitive reasoning and natural expressivity.","tags":null,"title":"Wafa Johal","type":"authors"},{"authors":["Yu (Carson) Liu"],"categories":null,"content":"Yu (Carson) Liu is a first year MPhil student at CSE UNSW. His thesis topic is on speech-driven gesture generation for social robots.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9b46976d335323fa7e06e738769f4018","permalink":"https://CHRI-Lab.github.io/author/yu-carson-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yu-carson-liu/","section":"authors","summary":"Yu (Carson) Liu is a first year MPhil student at CSE UNSW. His thesis topic is on speech-driven gesture generation for social robots.","tags":null,"title":"Yu (Carson) Liu","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://CHRI-Lab.github.io/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":["Ornnalin Phaijit","Mohammad Obaid","Wafa Johal","Claude Sammut"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638139208,"objectID":"e66bd679c88c1306a9ee1157ce365e51","permalink":"https://CHRI-Lab.github.io/publication/phaijit-obaid-johal-sammut-2022/","publishdate":"2021-11-28T22:40:07.499732Z","relpermalink":"/publication/phaijit-obaid-johal-sammut-2022/","section":"publication","summary":"","tags":[],"title":"A Functional Taxonomy of Visual Augmented Reality for Human-Robot Interaction","type":"publication"},{"authors":["Aida Amirova","Nazerke Rakhymbayeva","Elmira Yadollahi","Anara Sandygulova","Wafa Johal"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637921695,"objectID":"94f165d8cb059d39c1fbeda0960c5e23","permalink":"https://CHRI-Lab.github.io/publication/amirova-2021-nao/","publishdate":"2021-11-26T10:14:55.618881Z","relpermalink":"/publication/amirova-2021-nao/","section":"publication","summary":"The evolving field of human-robot interaction (HRI) necessitates that we better understand how social robots operate and interact with humans. This scoping review provides an overview of about 300 research works focusing on the use of the NAO robot from 2010 to 2020. This study presents one of the most extensive and inclusive pieces of evidence on the deployment of the humanoid NAO robot and its global reach. Unlike most reviews, we provide both qualitative and quantitative results regarding how NAO is being used and what has been achieved so far. We analyzed a wide range of theoretical, empirical, and technical contributions that provide multidimensional insights, such as general trends in terms of application, the robot capabilities, its input and output modalities of communication, and the human-robot interaction experiments that featured NAO (e.g. number and roles of participants, design, and the length of interaction). Lastly, we derive from the review some research gaps in current state-of-the-art and provide suggestions for the design of the next generation of social robots.","tags":[],"title":"10 Years of Human-NAO Interaction Research: A Scoping Review","type":"publication"},{"authors":["Wafa Johal","Barbara Bruno","Jennifer K. Olsen","Mohamed Chetouani","Séverin Lemaignan","Anara Sandygulova"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637921695,"objectID":"83fccf4ca317c125a6857812c68565cc","permalink":"https://CHRI-Lab.github.io/publication/johal-2021-r-4-l/","publishdate":"2021-11-26T10:14:54.940065Z","relpermalink":"/publication/johal-2021-r-4-l/","section":"publication","summary":"The Robots for Learning workshop series aims at advancing the research topics related to the use of social robots in educational contexts. This year's half-day workshop follows on previous events in Human-Robot Interaction conferences focusing on efforts to dis-cuss potential benchmarks in design, methodology and evaluation of new robotics systems that help learners. In this 6th edition of the workshop, we will be investigating in particular methods from technologies for education and online learning. Since the past few months, online and remote learning has been put in place in several countries to cope with the health and safety measures due to theCovid-19 pandemic. In this workshop, we aim to discuss strategies to design robotics system able to provide embodied assistance to the remote learners and to demonstrate long-term learning effects.","tags":["\"robots for learning\"","\"human-robot interaction\"","\"robot in education\""],"title":"Robots for Learning - Learner-Centred Design","type":"publication"},{"authors":["Yu Liu","Gelareh Mohammadi","Yang Song","Wafa Johal"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637921695,"objectID":"84eac074b000ced196f37fccd894ec21","permalink":"https://CHRI-Lab.github.io/publication/liu-2021-speech/","publishdate":"2021-11-26T10:14:55.28584Z","relpermalink":"/publication/liu-2021-speech/","section":"publication","summary":" Humans use gestures as a means of non-verbal communication. Often accompanying speech, these gestures have several purposes but in general, aim to convey an intended message to the receiver. Researchers have tried to develop systems to allow embodied agents to be better communicators when interacting with humans via using gestures. In this article, we present a scoping literature review of the methods and the metrics used to generate and evaluate co-speech gestures. After collecting a set of papers using a term search on the Scopus database, we analysed the content of these papers based on methodology (i.e., model, the dataset used), evaluation measures (i.e., objective and subjective) and limitations. The results indicate that data-driven approaches are used more frequently. In terms of evaluation measures, we found a trend of combining objective and subjective metrics, while no standards exist for either. This literature review provides an overview of the research in the area and, more specifically insights the trends and the challenges to be met in building a system to automatically generate gestures for embodied agents.","tags":["\"literature review\"","\"survey\"","\"robot\"","\"gesture generation\"","\"co-speech gestures\""],"title":"Speech-Based Gesture Generation for Robots and Embodied Agents: A Scoping Review","type":"publication"},{"authors":["Thibault Asselborn","Wafa Johal","Bolat Tleubayev","Zhanel Zhexenova","Pierre Dillenbourg","Catherine McBride","Anara Sandygulova"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637921694,"objectID":"747406731679275cef88985b14a2c50f","permalink":"https://CHRI-Lab.github.io/publication/asselborn-2021-transferability/","publishdate":"2021-11-26T10:14:54.598225Z","relpermalink":"/publication/asselborn-2021-transferability/","section":"publication","summary":"","tags":[],"title":"The transferability of handwriting skills: from the Cyrillic to the Latin alphabet","type":"publication"},{"authors":["Daisy Ingle","Nadine Marcus","Wafa Johal"],"categories":[],"content":"PDF  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611034569,"objectID":"8ab80eb36ce7574780eb416490e6df6f","permalink":"https://CHRI-Lab.github.io/publication/ingle-2021/","publishdate":"2021-01-19T05:36:09.494485Z","relpermalink":"/publication/ingle-2021/","section":"publication","summary":"Previous research in psychology has found that human faces have the capability of being more distracting under high perceptual load conditions compared to non-face objects.  This project aims to assess the distracting potential of robot faces based on their human-likeliness. As a first step, this paper reports on our initial findings based on an online study. We used a letter search task where participants had to search for a target letter within a circle of 6 letters, whilst an irrelevant distractor image was also present.  The results of our experiment replicated previous results with human faces and non-face objects. Additionally, in the tasks where the irrelevant distractors are images of robot faces, the human-likeness of the robot influenced the response time (RT).  Interestingly, the robot Alter produced results significantly different than all other distractor robots.  The outcome of this is a distraction model related to human-likeness of robots. Our results show the impact of anthropomorphism on distracting potential and thus should be taken into account when designing robots.","tags":["\"anthropomorphism\"","\"robot design\"","\"human-likeness\"","\"perception load\""],"title":"The Valley of non-Distraction: Effect of Robot's Human-likeness on Perception Load","type":"publication"},{"authors":["Mohammad Obaid","Wafa Johal","Omar Mubin"],"categories":null,"content":"  ","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"c0ab36859939c48522e46924828ba956","permalink":"https://CHRI-Lab.github.io/publication/obaid_hai_2020/","publishdate":"2020-11-10T04:00:36.444312Z","relpermalink":"/publication/obaid_hai_2020/","section":"publication","summary":"Domestic robotic entities are on the rise, out of which, domestic drones are taking place in our society as one of the upcoming interactive technologies that we will see in our daily lives. In this paper, we scope for research literature that addresses the use of domestic drones within our environments to understand the current usage as well as identifying future research directions. After performing a search based collection of relevant papers in the ACM digital library (N=61 papers), we analysed the drone's application areas, their interaction modalities, the target users, and the level of autonomy of the proposed systems. The results show interesting trends in the modalities of interaction (visual projection combined with hand/foot gestures) as well as important research gaps such as child-drone interaction, and the use of drones for healthcare or education, given that currently most use cases for domestic drones are generic in nature. ","tags":["drone"],"title":"Domestic Drones: Context of Use in Research Literature","type":"publication"},{"authors":null,"categories":null,"content":"Super excited about the new SydCHI local chapter. Stay tune!\n","date":1593043200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593043200,"objectID":"8a3210f61f3d216086c5f06a9cd5c134","permalink":"https://CHRI-Lab.github.io/post/sydchi_accepted/","publishdate":"2020-06-25T00:00:00Z","relpermalink":"/post/sydchi_accepted/","section":"post","summary":"Super excited about the new SydCHI local chapter. Stay tune!","tags":null,"title":"The SydCHI Local Chapter just got accepted","type":"post"},{"authors":["Isabel Neto","Wafa Johal","Marta Couto","Hugo Nicolau","Ana Paiva","Arzu Guneysu"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"5b4411c726fa0049025d4bd5a28e7ace","permalink":"https://CHRI-Lab.github.io/publication/neto-using-2020/","publishdate":"2020-04-28T04:00:36.444312Z","relpermalink":"/publication/neto-using-2020/","section":"publication","summary":"Geometry and handwriting rely heavily on the visual representation of basic shapes. It can become challenging for students with visual impairments to perceive these shapes and understand complex spatial constructs. For instance, knowing how to draw is highly dependent on spatial and temporal components, which are often inaccessible to children with visual impairments. Hand-held robots, such as the Cellulo robots, open unique opportunities to teach drawing and writing through haptic feedback. In this paper, we investigate how these tangible robots could support inclusive, collaborative learning activities, particularly for children with visual impairments. We conducted a user study with 20 pupils with and without visual impairments, where they engaged in multiple drawing activities with tangible robots. We contribute novel insights on the design of children-robot interaction, learning shapes and letters, children engagement, and responses in a collaborative scenario that address the challenges of inclusive learning. ","tags":["tangible","cellulo"],"title":"Using Tabletop Robots to promote Inclusive Classroom Experiences","type":"publication"},{"authors":null,"categories":null,"content":"Our international team received the best demo award at @HRI2020 @dillenbo @AnaraSandy! #SNSF #CoKaz http://humanrobotinteraction.org/2020/best-paper-awards/\n","date":1590364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590364800,"objectID":"36855b11a301badce7a2efa7c1f26665","permalink":"https://CHRI-Lab.github.io/post/hri2020_demo_award/","publishdate":"2020-05-25T00:00:00Z","relpermalink":"/post/hri2020_demo_award/","section":"post","summary":"Our international team received the best demo award at @HRI2020 @dillenbo @AnaraSandy! #SNSF #CoKaz http://humanrobotinteraction.org/2020/best-paper-awards/","tags":["cowriting_kazakh"],"title":"Best Demo Award at HRI 2020!","type":"post"},{"authors":["Quentin Chibaudel","Wafa Johal","Bernard Oriola","Marc J-M Macé","Pierre Dillenbourg","Valérie Tartas","Christophe Jouffrais"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610789934,"objectID":"f01d75f1fe4cd29d730121295fff91d6","permalink":"https://CHRI-Lab.github.io/publication/chibaudel-2020/","publishdate":"2021-01-16T09:38:54.382277Z","relpermalink":"/publication/chibaudel-2020/","section":"publication","summary":"Tangible User Interfaces (TUI) have been found to be relevant tools for collaborative learning by providing a shared workspace and enhancing joint visual attention. Researchers have explored the use of TUIs in a variety of curricular activities and found them particularly interesting for spatial exploration. However, very few studies have explored how TUIs could be used as a collaborative medium for people with visual impairments (VIs). In this study, we investigated the effect of tangible interaction (a small tangible robot) in a spatial collaborative task (a treasure hunt) involving two people with VIs. The aim was to evaluate the impact of the design of the TUI on the collaboration and the strategies used to perform the task. The experiment involved six dyads of people with VIs. The results showed that the collaboration was impacted by the interaction design and open interesting perspectives on the design of collaborative games for people with VIs.","tags":["\"Spatial cognition\"","\"Game\"","\"Wayfinding\"","\"Robots\"","\"Non-visual interaction\"","\"Maps\"","\"Learning\"","\"Haptics\"","\"3D model\""],"title":"\\\"If You've Gone Straight, Now, You Must Turn Left\\\" - Exploring the Use of a Tangible Interface in a Collaborative Treasure Hunt for People with Visual Impairments","type":"publication"},{"authors":["Zhanel Zhexenova","Aida Amirova","Manshuk Abdikarimova","Kuanysh Kudaibergenov","Nurakhmet Baimakhan","Bolat Tleubayev","Thibault Asselborn","Wafa Johal","Pierre Dillenbourg","Anna CohenMiller","Anara Sandygulova"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610789934,"objectID":"f4286ce525bd2507338ff84d4ccbe045","permalink":"https://CHRI-Lab.github.io/publication/zhanel-2020/","publishdate":"2021-01-16T09:38:54.455782Z","relpermalink":"/publication/zhanel-2020/","section":"publication","summary":"This research occurred in a special context where Kazakhstan's recent decision to switch from Cyrillic to the Latin-based alphabet has resulted in challenges connected to teaching literacy, addressing a rare combination of research hypotheses and technical objectives about language learning. Teachers are not necessarily trained to teach the new alphabet, and this could result in a challenge for children with learning difficulties. Prior research studies in Human-Robot Interaction (HRI) have proposed the use of a robot to teach handwriting to children (Hood et al., 2015; Lemaignan et al., 2016). Drawing on the Kazakhstani case, our study takes an interdisciplinary approach by bringing together smart solutions from robotics, computer vision areas, and educational frameworks, language, and cognitive studies that will benefit diverse groups of stakeholders. In this study, a human-robot interaction application is designed to help primary school children learn both a newly-adopted script and also its handwriting system. The setup involved an experiment with 62 children between the ages of 7–9 years old, across three conditions: a robot and a tablet, a tablet only, and a teacher. Based on the paradigm—learning by teaching—the study showed that children improved their knowledge of the Latin script by interacting with a robot. Findings reported that children gained similar knowledge of a new script in all three conditions without gender effect. In addition, children's likeability ratings and positive mood change scores demonstrate significant benefits favoring the robot over a traditional teacher and tablet only approaches.","tags":[],"title":"A Comparison of Social Robot to Tablet and Teacher in a New Script Learning Context","type":"publication"},{"authors":["Thomas Gargot","Thibault Asselborn","Hugues Pellerin","Ingrid Zammouri","Salvatore M. Anzalone","Laurence Casteran","Wafa Johal","Pierre Dillenbourg","David Cohen","Caroline Jolly"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604290917,"objectID":"5c782e3df399b09339850ce784c0dc85","permalink":"https://CHRI-Lab.github.io/publication/gargot-2020-acquisition/","publishdate":"2020-11-02T04:21:57.234702Z","relpermalink":"/publication/gargot-2020-acquisition/","section":"publication","summary":"Handwriting is a complex skill to acquire and it requires years of training to be mastered. Children presenting dysgraphia exhibit difficulties automatizing their handwriting. This can bring anxiety and can negatively impact education. 280 children were recruited in schools and specialized clinics to perform the Concise Evaluation Scale for Children’s Handwriting (BHK) on digital tablets. Within this dataset, we identified children with dysgraphia. Twelve digital features describing handwriting through different aspects (static, kinematic, pressure and tilt) were extracted and used to create linear models to investigate handwriting acquisition throughout education. K-means clustering was performed to define a new classification of dysgraphia. Linear models show that three features only (two kinematic and one static) showed a significant association to predict change of handwriting quality in control children. Most kinematic and statics features interacted with age. Results suggest that children with dysgraphia do not simply differ from ones without dysgraphia by quantitative differences on the BHK scale but present a different development in terms of static, kinematic, pressure and tilt features. The K-means clustering yielded 3 clusters (Ci). Children in C1 presented mild dysgraphia usually not detected in schools whereas children in C2 and C3 exhibited severe dysgraphia. Notably, C2 contained individuals displaying abnormalities in term of kinematics and pressure whilst C3 regrouped children showing mainly tilt problems. The current results open new opportunities for automatic detection of children with dysgraphia in classroom. We also believe that the training of pressure and tilt may open new therapeutic opportunities through serious games.","tags":[],"title":"Acquisition of handwriting in children with and without dysgraphia: A computational approach","type":"publication"},{"authors":["Anara Sandygulova","Wafa Johal","Zhanel Zhexenova","Bolat Tleubayev","Aida Zhanatkyzy","Aizada Turarova","Zhansaule Telisheva","Anna CohenMiller","Thibault Asselborn","Pierre Dillenbourg"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"530a07ea8569051cc9f48fe2ae922202","permalink":"https://CHRI-Lab.github.io/publication/sandygulova-hri-2020/","publishdate":"2020-03-09T03:55:01.363162Z","relpermalink":"/publication/sandygulova-hri-2020/","section":"publication","summary":"In the Republic of Kazakhstan, the transition from Cyrillic to Latin alphabet raises challenges to training an entire population in writing the new script. This paper presents a CoWriting Kazakh system, an extension of the existing CoWriter system, aiming to implement an autonomous social robot that would assist children in transition from the old Cyrillic alphabet to a new Latin alphabet. With the aim to investigate which learning strategy yields better learning gains, we conducted an experiment with 67 children, aged 8-11 years old, who interacted with a robot in a CoWriting Kazakh learning scenario. Participants were asked to teach a humanoid NAO robot how to write Kazakh words using one of the scripts, Latin or Cyrillic. We hypothesized that a scenario in which the child is asked to mentally convert the word to Latin would be more effective than having the robot perform conversion itself. Results show that the CoWriter was successfully applied to this new script-switching task. The findings also suggest interesting gender differences in the preferred method of learning with the robot.","tags":["cowriting_kazakh"],"title":"CoWriting Kazakh: Learning a New Script with a Robot","type":"publication"},{"authors":["Bolat Tleubayev","Zhanel Zhexenova","Thibault Asselborn","Wafa Johal","Pierre Dillenbourg","Anara Sandygulova"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637921694,"objectID":"67d918c1a1b28e99d950dee03e62aa1e","permalink":"https://CHRI-Lab.github.io/publication/bolat-hri-2020-demo/","publishdate":"2021-11-26T10:14:54.25876Z","relpermalink":"/publication/bolat-hri-2020-demo/","section":"publication","summary":"This interdisciplinary project aims to assess and manage the risks relating to the transition of Kazakh language from Cyrillic to Latin in Kazakhstan in order to address challenges of a) teaching and motivating children to learn a new script and its associated handwriting, and b) training and providing support for all demographic groups, in particular senior generation. We present the system demonstration that proposes to assist and motivate children to learn a new script with the help of a humanoid robot and a tablet with stylus.","tags":["\"language learning\"","\"social robot\"","\"handwriting recognition\""],"title":"CoWriting Kazakh: Learning a New Script with a Robot - Demonstration","type":"publication"},{"authors":["Zhanel Zhexenova","Bolat Tleubayev","Aida Zhanatkyzy","Aizada Turarova","Zhansaule Telisheva","Wafa Johal","Thibault Asselborn","Pierre Dillenbourg","Anara Sandygulova"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"40b7fc1a8870bfc014c81bdf6a9f6941","permalink":"https://CHRI-Lab.github.io/publication/zhanel-hri-2020-video/","publishdate":"2020-03-09T03:42:41.109295Z","relpermalink":"/publication/zhanel-hri-2020-video/","section":"publication","summary":"","tags":["cowriting_kazakh"],"title":"CoWriting Kazakh: Learning a New Script with a Robot - Video ","type":"publication"},{"authors":["Elmira Yadollahi","Marta Couto","Wafa Johal","Pierre Dillenbourg","Ana Paiva"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604290917,"objectID":"788eaf76220a095a30a022b68b5b2619","permalink":"https://CHRI-Lab.github.io/publication/yadollahi-2020-exploring/","publishdate":"2020-11-02T04:21:57.079818Z","relpermalink":"/publication/yadollahi-2020-exploring/","section":"publication","summary":"Perspective taking is an important skill to have and learn, which can be applied in many different domains and disciplines. While the ability to recognize other’s perspective develops in humans from childhood and solidifies during school years, it needs to be developed in robotic and artificial agents’ cognitive framework. In our quest to develop a cognitive model of perspective taking for agents and robots in educational contexts, we designed a task that requires the players (e.g., child and robot) to take the perspective of another, in order to complete and win the task successfully. In a preliminary study to test the system, we were able to evaluate children’s performance over four different age groups by focusing on their performance during the interaction with the robot. By analyzing children’s performance, we were able to make some assumptions about children’s understanding of the game and select the appropriate age group to participate in the main study.","tags":[],"title":"Exploring the Role of Perspective Taking in Educational Child-Robot Interaction","type":"publication"},{"authors":["Arzu Guneysu Ozgur","Maximilian Jonas Wessel","Jennifer Kaitlyn Olsen","Wafa Johal","Ayberk Özgür","Friedhelm C. Hummel","Pierre Dillenbourg"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"ff43b3272c201c032686bf6b10081eef","permalink":"https://CHRI-Lab.github.io/publication/guneysu-ozgur-gamified-2020/","publishdate":"2020-03-09T03:42:41.109729Z","relpermalink":"/publication/guneysu-ozgur-gamified-2020/","section":"publication","summary":"Background: The increasing lifespan and the resulting change of our expectations in later life stages are dependent on a good health state. This emphasizes the development of strategies to further healthy ageing. One important aspect of good health in later life stages is sustained skilled motor function. Objective: Here, we tested the effectiveness of robotic upper limb motor training in a game-like scenario assessing game-based learning and its transfer potential. Methods: Thirty-six healthy participants (n = 18 elderly participants, n = 18 young controls) trained with a Pacman-like game using a hand-held Cellulo robot on two consecutive days. The game-related movements were conducted on a printed map displaying a maze and targets that had to be collected. Gradually, the task difficulty was adjusted between games by modifying or adding different game elements (e.g., speed and number of chasing ghosts, additional rules, haptic feedback). Transfer was assessed by scoring simple robot manipulation on two different trajectories. Results: Elderly participants were able to improve their game performance over time (t(874) = 2.97, p textless .01). The applied game-elements had similar effects on both age groups. Importantly, the game-based learning transferred to simple robot manipulation, which resembles activities of daily life. Only minor age-related differences were present (smaller overall learning gain, different effect of the wall crash penalty rule in the elderly group). Conclusions: Gamified motor training with the Cellulo system has the potential to translate into an efficient, relatively low-cost robotic motor training tool for promoting upper limb function to promote healthy aging.","tags":["tangible robots","Task Performance and Analysis","Aging","Gamified activities","Healthy Ageing","motor learning","Robotic exercise","Transfer Learning"],"title":"Gamified Motor Training with Tangible Robots in Older Adults: a Feasibility Study and Comparison with Young","type":"publication"},{"authors":["Arzu Guneysu Ozgur","Ayberk Özgür","Thibault Asselborn","Wafa Johal","Elmira Yadollahi","Barbara Bruno","Melissa Skweres","Pierre Dillenbourg"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"38d58b1363e51d58966c348fd4a15d22","permalink":"https://CHRI-Lab.github.io/publication/guneysu-ozgur-iterative-2020/","publishdate":"2020-03-09T03:42:41.109531Z","relpermalink":"/publication/guneysu-ozgur-iterative-2020/","section":"publication","summary":"In this article, we investigate the role of interactive haptic-enabled tangible robots in supporting the learning of cursive letter writing for children with attention and visuomotor coordination issues. We focus on the two principal aspects of handwriting that are linked to these issues: Visual perception and visuomotor coordination. These aspects respectively enhance two features of letter representation in the learner's mind in particular, namely the shape (grapheme) and the dynamics (ductus) of the letter, which constitute the central learning goals in our activity. Building upon an initial design tested with 17 healthy children in a preliminary school, we iteratively ported the activity to an occupational therapy context in 2 different therapy centers, in the context of 3 different summer school camps involving a total of 12 children having writing difficulties. The various iterations allowed us to uncover insights about the design of robot-enhanced writing activities for special education, specifically highlighting the importance of ease of modification of the duration of an activity as well as of adaptable frequency, content, flow, and game-play and of providing a range of evaluation test alternatives. Results show that the use of robot-assisted handwriting activities could have a positive impact on the learning of the representation of letters in the context of occupational therapy (V = 1449, p textless 0.001, r=0.42). Results also highlight how the design changes made across the iterations affected the outcomes of the handwriting sessions, such as the evaluation of the performances, monitoring of the performances, and the connectedness of the handwriting.","tags":["tangible robots","handwriting","Gamified activities","Haptic interface","interactive   learning environments","iterative design","Occupational thearpy","Robots for education"],"title":"Iterative Design and Evaluation of a Tangible Robot-Assisted Handwriting Activity for Special Education","type":"publication"},{"authors":["T Gargot","D Archambault","M Chetouani","D Cohen","W Johal","SM Anzalone"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610789934,"objectID":"523b54ceb9b6dd5a4173735be0149ad5","permalink":"https://CHRI-Lab.github.io/publication/gargot-2020-p/","publishdate":"2021-01-16T09:38:54.301115Z","relpermalink":"/publication/gargot-2020-p/","section":"publication","summary":"","tags":[],"title":"P. 114 Automatic assessment of motors impairments in autism spectrum disorders: a systematic review","type":"publication"},{"authors":["Wafa Johal"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604290917,"objectID":"d689616ba8d9b3113f9752188052b110","permalink":"https://CHRI-Lab.github.io/publication/johal-2020-research/","publishdate":"2020-11-02T04:21:56.913233Z","relpermalink":"/publication/johal-2020-research/","section":"publication","summary":" **Purpose of Review**\n\nWith the growth in the number of market-available social robots, there is an increasing interest in research on the usage of social robots in education. This paper proposes a summary of trends highlighting current research directions and potential research gaps for social robots in education. We are interested in design aspects and instructional setups used to evaluate social robotics system in an educational setting.\n\n**Recent Findings**\n\nThe literature demonstrates that as the field grows, setup, methodology, and demographics targeted by social robotics applications seem to settle and standardize—a tutoring Nao robot with a tablet in front of a child seems the stereotypical social educational robotics setup.\n\n**Summary**\n\nAn updated review on social robots in education is presented here. We propose, first, an analysis of the pioneering works in the field. Secondly, we explore the potential for education to be the ideal context to investigate central human-robot interaction research questions. A trend analysis is then proposed demonstrating the potential for educational context to nest impactful research from human-robot interaction. ","tags":[],"title":"Research Trends in Social Robots for Learning","type":"publication"},{"authors":["Wafa Johal","Yu Peng","Haipeng Mi"],"categories":[],"content":"  ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610789934,"objectID":"abca8b1f66fac3a0a1af4ce9f4d0025c","permalink":"https://CHRI-Lab.github.io/publication/johal-2020-swarm/","publishdate":"2021-01-16T09:38:53.98659Z","relpermalink":"/publication/johal-2020-swarm/","section":"publication","summary":"This study reviews published scientific literature on the use of swarm robots for education purposes in the last ten years. It focuses on user studies involving robotics swarm in order to identify the potential contributions of the incorporation of swarm robots as an educational tool and insight future research. We consider here the appearance of swarm robots, the curriculum of the experimental task and the interaction modalities between learners and robots. The outcomes of the literature review are discussed in terms of their existing challenges and opportunities for guiding researchers, educators, and practitioners.","tags":["\"educational tool\"","\"swarm robots\"","\"literature review\"","\"learning\"","\"multi-robot\""],"title":"Swarm Robots in Education: A Review of Challenges and Opportunities","type":"publication"},{"authors":null,"categories":null,"content":"","date":1575072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575072000,"objectID":"dd6e6808c6069da4c1f5843f3cee6959","permalink":"https://CHRI-Lab.github.io/post/hri2020_accepted/","publishdate":"2019-11-30T00:00:00Z","relpermalink":"/post/hri2020_accepted/","section":"post","summary":"","tags":["cowriter","handwriting","cowriting-kazakh"],"title":"[HRI2020 Paper Accepted!] on Transposing CoWriter to Learning the New Kazakh Alphabet","type":"post"},{"authors":null,"categories":null,"content":"I am hounoured that the 27th International Conference on Computers in Education awarded me with the Best Reviewer Award.\nhttp://ilt.nutn.edu.tw/icce2019/04_Best%20Reviewer%20Award.html\n","date":1571702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571702400,"objectID":"51a7092301dda9b3356940fb9759629f","permalink":"https://CHRI-Lab.github.io/post/best_reviewer_icce2019/","publishdate":"2019-10-22T00:00:00Z","relpermalink":"/post/best_reviewer_icce2019/","section":"post","summary":"I am hounoured that the 27th International Conference on Computers in Education awarded me with the Best Reviewer Award.\nhttp://ilt.nutn.edu.tw/icce2019/04_Best%20Reviewer%20Award.html","tags":null,"title":"Best Reviewer Award for ICCE","type":"post"},{"authors":null,"categories":null,"content":"I was very glad to meet my colleagues and friends at RoMan. We presented four papers and enjoyed a day off in Jaipur.\n","date":1571011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571011200,"objectID":"f2f08ecc96b447fef557b17523f79ed3","permalink":"https://CHRI-Lab.github.io/post/roman2019/","publishdate":"2019-10-14T00:00:00Z","relpermalink":"/post/roman2019/","section":"post","summary":"I was very glad to meet my colleagues and friends at RoMan. We presented four papers and enjoyed a day off in Jaipur.","tags":null,"title":"4 Full Papers Presented @RoMan 2019, New Delhi!","type":"post"},{"authors":null,"categories":null,"content":"I was very glad to meet my colleagues and friends virtually at RoMan2020.\n","date":1571011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571011200,"objectID":"624135077ff2d3fcb455154db7d0df9c","permalink":"https://CHRI-Lab.github.io/post/allohaptic2020/","publishdate":"2019-10-14T00:00:00Z","relpermalink":"/post/allohaptic2020/","section":"post","summary":"I was very glad to meet my colleagues and friends virtually at RoMan2020.","tags":null,"title":"Paper accepted! Allohaptic","type":"post"},{"authors":["Wafa Johal","Olguta Robu","Amaury Dame","Stephane Magnenat","Francesco Mondada"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"424d8a36948decefec7ade90164788f0","permalink":"https://CHRI-Lab.github.io/publication/johal-augmented-2019/","publishdate":"2020-03-09T04:27:29.927544Z","relpermalink":"/publication/johal-augmented-2019/","section":"publication","summary":"In recent years, robots have been surfing on a trendy wave as standard devices for teaching programming. The tangibility of robotics platforms allows for collaborative and interactive learning. Moreover, with these robot platforms, we also observe the occurrence of a shift of visual attention from the screen (on which the programming is done) to the physical environments (i.e. the robot). In this paper, we describe an experiment aiming at studying the effect of using augmented reality (AR) representations of sensor data in a robotic learning activity. We designed an AR system able to display in real-time the data of the Infra-Red sensors of the Thymio robot. In order to evaluate the impact of AR on the learner's understanding on how these sensors worked, we designed a pedagogical lesson that can run with or without the AR rendering. Two different age groups of students participated in this between-subject experiment, counting a total of 74 children. The tests were the same for the experimental (AR) and control group (no AR). The exercises differed only through the use of AR. Our results show that AR was worth being used for younger groups dealing with difficult concepts. We discuss our findings and propose future works to establish guidelines for designing AR robotic learning sessions.","tags":["AR rendering","AR robotic learning sessions","AR system","augmented reality","Augmented Reality","augmented reality representations","augmented robotics","collaborative learning","computer aided instruction","computer science education","control group","Education","educational robots","experimental group","human-robot interaction","infrared detectors","infrared sensors","interactive learning","interactive systems","Optics","programming","programming teaching","robot platforms","robotic learning activity","Robotics","robotics platforms","sensor data","standard devices","teaching","Thymio robot","trendy wave","visual attention"],"title":"Augmented Robotics for Learners: A Case Study on Optics","type":"publication"},{"authors":null,"categories":null,"content":"The iReCheck project submitted last April will be funded by the French ANR and the Swiss NSF. The goal of the project is to explore the learning by teaching sceanrio proposed in the cowriter project in with both performance and social adaptation, and with TD learners and learners with NDD. The project brings educational expertise of the EPFL team with the NDD expertise of the French partners to build a system that can suit all types of learners.\n","date":1568505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568505600,"objectID":"cf4f04d62798a5e0cf0cdda3df514596","permalink":"https://CHRI-Lab.github.io/post/irechech-accepted/","publishdate":"2019-09-15T00:00:00Z","relpermalink":"/post/irechech-accepted/","section":"post","summary":"The iReCheck project submitted last April will be funded by the French ANR and the Swiss NSF. The goal of the project is to explore the learning by teaching sceanrio proposed in the cowriter project in with both performance and social adaptation, and with TD learners and learners with NDD. The project brings educational expertise of the EPFL team with the NDD expertise of the French partners to build a system that can suit all types of learners.","tags":null,"title":"The iReCheck project accepted!","type":"post"},{"authors":null,"categories":null,"content":"Officially starting at @UNSWEngineering today. Looking forward to this exciting adventure! Stay tuned, Phd job offers coming soon.\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"b5ad825354e4a25ef4c7274425cefdcf","permalink":"https://CHRI-Lab.github.io/post/unsw-start/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/unsw-start/","section":"post","summary":"Officially starting at @UNSWEngineering today. Looking forward to this exciting adventure! Stay tuned, Phd job offers coming soon.","tags":null,"title":"First day at UNSW as Lecturer (Assistant Prof)","type":"post"},{"authors":["Thibault Asselborn","Kshitij Sharma","Wafa Johal","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"9c8942742600cb76c8e583246de9d4f1","permalink":"https://CHRI-Lab.github.io/publication/asselborn-2019-bridging/","publishdate":"2019-09-22T11:07:11.716669Z","relpermalink":"/publication/asselborn-2019-bridging/","section":"publication","summary":"In this article, we present a multi-level time scales framework for the analysis of human-robot interaction (HRI). Such a framework allows HRI scientists to model the inter-relation between measures and factors of an experiment. Our final goal with the introduction of this framework is to unify scientific practice in the HRI community for better reproducibility. Our new approach transposes Newell’s framework of human actions to model human-robot interaction. Measures from the interaction are sorted into categories (time scales) corresponding to the temporal constraints proposed by Newell. According to this sorting, a bottom-up or top-down analysis can then be performed to correlate variables which allows a better understanding and explanation of the interaction. The utilization of our method within two experimental use cases is then presented. The first one, a child-robot interaction, involves two robots and one child playing a memory game. The second is based on an analysis of the PInSoRo dataset, involving 30 child-robot pairs in a freeplay interaction. Finally, we introduce clear guidelines to re-use the framework.","tags":null,"title":"Bridging Multilevel Time Scales in HRI: An Analysis Framework","type":"publication"},{"authors":["Anton Kim","Meruyert Omarova","Adil Zhaksylyk","Thibault Asselborn","Wafa Johal","Pierre Dillenbourg","Anara Sandygulova"],"categories":null,"content":"   ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e10a4f9d1b2129ca2af46668088d6671","permalink":"https://CHRI-Lab.github.io/publication/kim-cowriting-2019/","publishdate":"2020-03-09T03:42:41.108198Z","relpermalink":"/publication/kim-cowriting-2019/","section":"publication","summary":"In the Republic of Kazakhstan, the transition from Cyrillic towards Latin alphabet raises challenges to teach the whole population in writing the new script. This paper presents a CoWriting Kazakh system that aims to implement an autonomous behavior of a social robot that would assist children in learning a new script. Considering the fact that the current generation of primary school children have to be fluent in both Kazakh scripts, this exploratory study aims to investigate which learning approach provides better effect. Participants were asked to teach a humanoid robot NAO how to write Kazakh words using one of the scripts, Latin vs Cyrillic. We hypothesize that it is more effective when a child mentally converts the word to Latin in comparison to having the robot perform conversion itself. The findings reject this hypothesis, but further research is needed as it is suggested that the way the pre-test was performed might have caused the obtained results.","tags":["cowriting_kazakh"],"title":"CoWriting Kazakh: Transitioning to a New Latin Script using Social Robots","type":"publication"},{"authors":["Arzu Guneysu Ozgur","Maximilian J Wessel","Thibault Asselborn","Jennifer K Olsen","Wafa Johal","Ayberk Ozgur","Friedhelm C Hummel","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"7bf11835c181ed178e041baf7c0e21bd","permalink":"https://CHRI-Lab.github.io/publication/ozgur-designing-2019/","publishdate":"2020-03-09T03:42:41.108477Z","relpermalink":"/publication/ozgur-designing-2019/","section":"publication","summary":"For successful rehabilitation of a patient after a stroke or traumatic brain injury, it is crucial that rehabilitation activities are motivating, provide feedback and have a high rate of repetitions. Advancements in recent technologies provide solutions to address these aspects where needed. Additionally, through the use of gamification, we are able to increase the motivation for participants. However, many of these systems require complex set-ups, which can be a big challenge when conducting rehabilitation in a home-based setting. To address the lack of simple rehabilitation tools for arm function for a home-based application, we previously developed a system, Cellulo for rehabilitation, that is comprised of paper-supported tangible robots that are orchestrated by applications deployed on consumer tablets. These components enable different features that allow for gamification, easy setup, portability, and scalability. To support the configuration of game elements to patients' level of motor skills and strategies, their motor trajectories need to be classified. In this paper, we investigate the classification of different motor trajectories and how game elements impact these in unimpaired, healthy participants. We show that the manipulation of certain game elements do have an impact on motor trajectories, which might indicate that it is possible to adapt the arm remediation of patients by configuring game elements. These results provide a first step towards providing adaptive rehabilitation based upon patients' measured trajectories.","tags":null,"title":"Designing Configurable Arm Rehabilitation Games: How Do Different Game Elements Affect User Motion Trajectories?","type":"publication"},{"authors":["Laila El Hamamsy","Wafa Johal","Thibault Asselborn","Jauwairia Nasir","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e518def071be9c71aa150bc89291eef0","permalink":"https://CHRI-Lab.github.io/publication/el-hamamsy-learning-2019/","publishdate":"2019-10-09T03:42:41.108908Z","relpermalink":"/publication/el-hamamsy-learning-2019/","section":"publication","summary":"This paper presents the design of a novel and engaging collaborative learning activity for handwriting where a group of participants simultaneously tutor a Nao robot. This activity was intended to take advantage of both collaborative learning and the learning by teaching paradigm to improve children’s meta-cognition (perception of their own skills). Multiple engagement probes were integrated into the activity as a first step towards fostering long term interactions. As a lot of research targets social interactions, the goal here was to determine whether an engagement strategy focused on the task could be as, or more efficient than one focused on social interactions and participants’ introspection. To that effect, two engagement strategies were implemented. They differed in content but used the same multi-modal design in order to increase participants’ meta-cognitive reflection, once on the task and performances, and once on participants’ enjoyment and emotions. Both strategies were compared to a baseline by probing and assessing engagement at the individual and group level, along the behavioural, emotional and cognitive dimensions, in a between subject experiment with 12 groups of children. The experiments showed that the collaborative task pushed the children to adapt their manner of writing to the group, even though the adopted solution was not always correct. Furthermore, there was no significant difference between the strategies in terms of behaviour on task (behavioural engagement), satisfaction (emotional engagement) or performance (cognitive engagement) as the group dynamics had a stronger impact on the outcome of the collaborative teaching task. Therefore, the task and social engagement strategies can be considered as efficient in the context of collaboration.","tags":null,"title":"Learning By Collaborative Teaching: An Engaging Multi-Party CoWriter Activity","type":"publication"},{"authors":["Wafa Johal","Sonia Andersen","Morgane Chevalier","Ayberk Ozgur","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"  ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"91cbca55285a57db36853cae1bffec9a","permalink":"https://CHRI-Lab.github.io/publication/johal-2019-learning/","publishdate":"2019-09-22T11:07:11.716394Z","relpermalink":"/publication/johal-2019-learning/","section":"publication","summary":"Robots bring a new potential for embodied learning in classrooms. With our project, we aim to ease the task for teachers and to show the worth of tangible manipulation of robots in educational contexts. In this article, we present the design and the evaluation of two pedagogical activities prepared for a primary school teacher and targeting common misconceptions when learning reflective symmetry. The evaluation consisted of a comparison of remedial actions using haptic-enabled tangible robots with using regular geometrical tools in practical sessions. Sixteen 10 y.o. students participated in a between-subject experiment in a public school. We show that this training with the tangible robots helped the remediation of parallelism and perpendicularity related mistakes commonly made by students.  Our findings also suggest that the haptic modality of interaction is well suited to promote children’s abstraction of geometrical concepts from spatial representations.","tags":null,"title":"Learning Symmetry with Tangible Robots","type":"publication"},{"authors":["Ayberk Özgür","Séverin Lemaignan","Wafa Johal","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"263ad18439c7f7b051b5c33fdc36d9f4","permalink":"https://CHRI-Lab.github.io/publication/ozgur-2019-magnet/","publishdate":"2019-09-22T11:07:11.715019Z","relpermalink":"/publication/ozgur-2019-magnet/","section":"publication","summary":"","tags":null,"title":"Magnet-assisted ball drive","type":"publication"},{"authors":["Sina Shahmoradi","Jennifer K Olsen","Stian Haklev","Wafa Johal","Utku Norman","Jauwairia Nasir","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"71297461d162720b6f3685131bbe091b","permalink":"https://CHRI-Lab.github.io/publication/shahmoradi-2019-orchestration/","publishdate":"2019-09-22T11:07:11.717256Z","relpermalink":"/publication/shahmoradi-2019-orchestration/","section":"publication","summary":"Bringing robots into classrooms presents a new set of challenges for classroom management and teacher support compared to traditional technology-enhanced learning and has been left almost unexplored by the research community. In this paper, we present the opportunities and challenges of orchestrating Educational Robotics (ER) activities in classrooms. To support our discussion, we present a case study of 25 students working in pairs using handheld robots to engage in a computational thinking activity. While performing the activity, students’ behavioral information was sent from the robots to an orchestration dashboard that was used in a debriefing activity. Although this work is in its preliminary stages, it contributes to framing the challenges that need to be addressed to realistically scale-up usage of ER in classrooms.","tags":null,"title":"Orchestration of Robotic Activities in Classrooms: Challenges and Opportunities","type":"publication"},{"authors":["Thibault Asselborn","Thomas Gargot","Lukasz Kidzinski","Wafa Johal","David Cohen","Caroline Jolly","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"d7f0f339d199f344a651edfc99c2586f","permalink":"https://CHRI-Lab.github.io/publication/asselborn-2019-reply/","publishdate":"2019-09-22T11:07:11.71552Z","relpermalink":"/publication/asselborn-2019-reply/","section":"publication","summary":"","tags":null,"title":"Reply: Limitations in the creation of an automatic diagnosis tool for dysgraphia","type":"publication"},{"authors":["Jauwairia Nasir","Utku Norman","Wafa Johal","Jennifer Kaitlyn Olsen","Sina Shahmoradi","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8ee984ecafae9dcb65f01c8877d8b631","permalink":"https://CHRI-Lab.github.io/publication/nasir-2019-robot/","publishdate":"2019-09-22T11:07:11.716993Z","relpermalink":"/publication/nasir-2019-robot/","section":"publication","summary":"In this paper, we propose that the data generated by educational robots can be better used by applying learning analytics methods and techniques which can lead to a deeper understanding of the learners’ apprehension and behavior as well as refined guidelines for roboticists and improved interventions by the teachers. As a step towards this, we put forward analyzing behavior and task performance at team and/or individual levels by coupling robot data with the data from conventional methods of assessment through quizzes. Classifying learners/teams in the behavioral feature space with respect to the task performance gives insight into the behavior patterns relevant for high performance, which could be backed by feature ranking. As a use case, we present an open-ended learning activity using tangible haptic-enabled Cellulo robots in a classroom-level setting. The pilot study, spanning over approximately an hour, is conducted with 25 children in teams of two that are aged between 11-12. A linear separation is observed between the high and low performing teams where two of the behavioral features, namely number of distinct attempts and the visits to the destination, are found to be important. Although the pilot study in its current form has limitations, e.g. its low sample size, it contributes to highlighting the potential of the use of learning analytics in educational robotics.","tags":null,"title":"Robot Analytics: What Do Human-Robot Interaction Traces Tell Us About Learning?","type":"publication"},{"authors":["Wafa Johal","Anara Sandygulova","Jan De Wit","Mirjam De Haas","Brian Scassellati"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e4092c40a199788b4cf48c9571fe2bc4","permalink":"https://CHRI-Lab.github.io/publication/johal-2019-robots/","publishdate":"2019-09-22T11:07:11.715294Z","relpermalink":"/publication/johal-2019-robots/","section":"publication","summary":"","tags":null,"title":"Robots for Learning-R4L: Adaptive Learning","type":"publication"},{"authors":["Elmira Yadollahi","Wafa Johal","Joao Dias","Pierre Dillenbourg","Ana Paiva"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f567776878948bb883a897f551308091","permalink":"https://CHRI-Lab.github.io/publication/yadollahi-studying-2019/","publishdate":"2020-03-09T03:42:41.10912Z","relpermalink":"/publication/yadollahi-studying-2019/","section":"publication","summary":"The use of robots as peers is more and more studied in human-robot interaction with co-learning interactions being complex and rich involving cognitive, affective, verbal and non-verbal processes. We aim to study the co-learning interaction with robots in the light of perspective-taking; a cognitive dimension that is important for interaction, engagement, and learning of the child. This work-in-progress details one of the studies we are developing in understanding perspective-taking from the Piagetian point of view. The study tried to understand how changes in the robot's cognitive-affective state affect children's behavior, emotional state, and perception of the robot. The experiment details a scenario in which child and the robot take turn to play a game by instructing their counterpart to reach a goal. The interaction consists of a condition in which the robot expresses frustration when the child gives egocentric instructions. We manipulate the robot's emotional responses to the child's instructions as our independent variable. We hypothesize that children will try to change their perspective more when the robot expresses frustration and follow the instructions wrongly, e.g. does not understand their perspective. Moreover, in the frustration groups, we are interested to observe if children reciprocates the robot's behavior by showing frustration to the robot if it is egocentric. Consequently, we expect our analyses to help us to integrate a perspective-taking model in our robotic platform that can adapt its perspective according to educational or social aspect of the interaction.","tags":null,"title":"Studying the Effect of Robot Frustration on Children's Change of Perspective","type":"publication"},{"authors":["Konrad Zolna","Thibault Asselborn","Caroline Jolly","Laurence Casteran","Wafa Johal","Pierre Dillenbourg"," others"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"ba90b070c7f02f999cc6183a2b854929","permalink":"https://CHRI-Lab.github.io/publication/zolna-2019-dynamics/","publishdate":"2019-09-22T11:07:11.715786Z","relpermalink":"/publication/zolna-2019-dynamics/","section":"publication","summary":"Handwriting disorder (termed dysgraphia) is a far from a singular problem as nearly 8.6% of the population in France is considered dysgraphic. Moreover, research highlights the fundamental importance to detect and remediate these handwriting difficulties as soon as possible as they may affect a child's entire life, undermining performance and self-confidence in a wide variety of school activities. At the moment, the detection of handwriting difficulties is performed through a standard test called BHK. This detection, performed by therapists, is laborious because of its high cost and subjectivity. We present a digital approach to identify and characterize handwriting difficulties via a Recurrent Neural Network model (RNN). The child under investigation is asked to write on a graphics tablet all the letters of the alphabet as well as the ten digits. Once complete, the RNN delivers a diagnosis in a few milliseconds and demonstrates remarkable efficiency as it correctly identifies more than 90% of children diagnosed as dysgraphic using the BHK test. The main advantage of our tablet-based system is that it captures the dynamic features of writing -- something a human expert, such as a teacher, is unable to do. We show that incorporating the dynamic information available by the use of tablet is highly beneficial to our digital test to discriminate between typically-developing and dysgraphic children.","tags":null,"title":"The Dynamics of Handwriting Improves the Automated Diagnosis of Dysgraphia","type":"publication"},{"authors":["Wafa Johal","Alex Tran","Hala Khodr","Aykerk Ozgur","Pierre Dillenbourg"],"categories":null,"content":"  ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"14ebcbb029697ef75189bfc88c8bd127","permalink":"https://CHRI-Lab.github.io/publication/johal-2019-tip/","publishdate":"2019-12-12T10:12:15.828976Z","relpermalink":"/publication/johal-2019-tip/","section":"publication","summary":"While digital tools are more and more used in classrooms, teachers’ common practice remain to use photocopied documents to share and collect learning exercises from their students. With TIP (Tangible e-Ink Paper) we aim to explore the use of tangible manipulatives to interact with paper sheets and bridge between digital and paper traces of learning tasks. Featuring an e-Ink display, a paper-based localisation system and a wireless connection, TIPs is envisioned to be used as a versatile tool across various curriculum activities. In this paper, we present the design principle of TIPs and a first functional prototype. We conclude by future work in evaluating TIPs as a distributed sensor for teacher in their classroom, including learning scenario examples to illustrate our statements. ","tags":null,"title":"TIP: Tangible e-Ink Paper Manipulatives for Classroom Orchestration","type":"publication"},{"authors":["Arzu Guneysu Ozgur","Louis P Faucon","Pablo Maceira-Elvira","Maximilian J Wessel","Wafa Johal","Ayberk Özgür","Andéol Cadic-Melchior","Friedhelm C Hummel","Pierre Dillenbourg"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"59423486f13bfece013522077ea6808d","permalink":"https://CHRI-Lab.github.io/publication/ozgur-2019-towards/","publishdate":"2019-09-22T11:07:11.716077Z","relpermalink":"/publication/ozgur-2019-towards/","section":"publication","summary":"A key feature of a successful game is its ability to provide the player with an adequate level of challenge. However, the objective of difficulty adaptation in serious games is not only to maintain the player’s motivation by challenging, but also to ensure the completion of training objectives. This paper describes our proposed upper-limb rehabilitation game with tangible robots and investigates the effect of game elements and gameplay on the amount of the performed motion in several planes and percentage of failure by using the data from 33 unimpaired subjects who played 53 games within two consecutive days. In order to provide a more generic adaptation strategy in the future, we discretize the game area to circular zones. We then show the effect of changing these zones during gameplay on the activation of different muscles through EMG data in a pilot study. The study shows that it is possible to increase the challenge level by adding more active agents chasing the player and increasing the speed of these agents. However, only the increase in number of agents significantly increases the users’ motion on both planes. Analysis of player behaviors leads us to suggest that by adapting the behaviour of these active agents in specific zones, it is possible to change the trajectory of the user, and to provide a focus on the activation of specific muscles.","tags":null,"title":"Towards an Adaptive Upper Limb Rehabilitation Game with Tangible Robots","type":"publication"},{"authors":null,"categories":null,"content":"The CHILI Lab presented several demo at the Swiss Converntion Center. Always a great opportunity to meet with colleagues from the NCCR and robotics actor of the reagion.\n","date":1541548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541548800,"objectID":"12fdc4a16727a7923d8df241eeb936e7","permalink":"https://CHRI-Lab.github.io/post/nccr_id2019/","publishdate":"2018-11-07T00:00:00Z","relpermalink":"/post/nccr_id2019/","section":"post","summary":"The CHILI Lab presented several demo at the Swiss Converntion Center. Always a great opportunity to meet with colleagues from the NCCR and robotics actor of the reagion.","tags":null,"title":"Demos at NCCR Robotics Industry Days","type":"post"},{"authors":["Thibault Asselborn","Thomas Gargot","Lukasz Kidzinski","Wafa Johal","David Cohen","Caroline Jolly","Pierre Dillenbourg"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"030cceb3757b91071046240d889aa335","permalink":"https://CHRI-Lab.github.io/publication/asselborn-2018-automated/","publishdate":"2019-09-22T00:00:00Z","relpermalink":"/publication/asselborn-2018-automated/","section":"publication","summary":"The academic and behavioral progress of children is associated with the timely development of reading and writing skills. Dysgraphia, characterized as a handwriting learning disability, is usually associated with dyslexia, developmental coordination disorder (dyspraxia), or attention deficit disorder, which are all neuro-developmental disorders. Dysgraphia can seriously impair children in their everyday life and require therapeutic care. Early detection of handwriting difficulties is, therefore, of great importance in pediatrics. Since the beginning of the 20th century, numerous handwriting scales have been developed to assess the quality of handwriting. However, these tests usually involve an expert investigating visually sentences written by a subject on paper, and, therefore, they are subjective, expensive, and scale poorly. Moreover, they ignore potentially important characteristics of motor control such as writing dynamics, pen pressure, or pen tilt. However, with the increasing availability of digital tablets, features to measure these ignored characteristics are now potentially available at scale and very low cost. In this work, we developed a diagnostic tool requiring only a commodity tablet. To this end, we modeled data of 298 children, including 56 with dysgraphia. Children performed the BHK test on a digital tablet covered with a sheet of paper. We extracted 53 handwriting features describing various aspects of handwriting, and used the Random Forest classifier to diagnose dysgraphia. Our method achieved 96.6% sensibility and 99.2% specificity. Given the intra-rater and inter-rater levels of agreement in the BHK test, our technique has comparable accuracy for experts and can be deployed directly as a diagnostics tool.","tags":null,"title":"Automated human-level diagnosis of dysgraphia using a consumer tablet","type":"publication"},{"authors":["Thibault Asselborn","Arzu Guneysu","Khalil Mrini","Elmira Yadollahi","Ayberk Ozgur","Wafa Johal","Pierre Dillenbourg"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"a6cab791bd4c3281eff1ded37f6d97ca","permalink":"https://CHRI-Lab.github.io/publication/asselborn-2018-bringing/","publishdate":"2019-09-22T10:59:49.226312Z","relpermalink":"/publication/asselborn-2018-bringing/","section":"publication","summary":" In this paper, we present a robotic approach to improve the teaching of handwriting using the tangible, haptic-enabled and classroom-friendly Cellulo robots. Our efforts presented here are in line with the philosophy of the Cellulo platform: we aim to create a ready-to-use tool (i.e. a set of robot-assisted activities) to be used for teaching handwriting, one that is to coexist harmoniously with traditional tools and will contribute new added values to the learning process, complementing existing teaching practices.\nTo maximize our potential contributions to this learning process, we focus on two promising aspects of handwriting: the visual perception and the visual-motor coordination. These two aspects enhance in particular two sides of the representation of letters in the mind of the learner: the shape of the letter (the grapheme) and the way it is drawn, namely the dynamics of the letter (the ductus).\nWith these two aspects in mind, we do a detailed content analysis for the process of learning the representation of letters, which leads us to discriminate the specific skills involved in letter representation. We then compare our robotic method with traditional methods as well as with the combination of the two methods, in order to discover which of these skills can benefit from the use of Cellulo.\nAs handwriting is taught from age 5, we conducted our experiments with 17 five-year-old children in a public school. Results show a clear potential of our robot-assisted learning activities, with a visible improvement in certain skills of handwriting, most notably in creating the ductus of the letters, discriminating a letter among others and in the average handwriting speed.\nMoreover, we show that the benefit of our learning activities to the handwriting process increases when it is used after traditional learning methods. These results lead to the initial insights into how such a tangible robotic learning technology may be used to create cost-effective collaborative scenarios for the learning of handwriting. ","tags":null,"title":"Bringing letters to life: handwriting with haptic-enabled tangible robots","type":"publication"},{"authors":["Ayberk Özgür","Wafa Johal","Arzu Güneysu Özgür","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"131617ddeaf7d3212b31145a52ea8dc6","permalink":"https://CHRI-Lab.github.io/publication/ozgur-2018-declarative/","publishdate":"2019-09-22T11:07:11.714798Z","relpermalink":"/publication/ozgur-2018-declarative/","section":"publication","summary":"","tags":null,"title":"Declarative Physicomimetics for Tangible Swarm Application Development","type":"publication"},{"authors":["Arzu Guneysu Ozgur","Maximilian Jonas Wessel","Wafa Johal","Kshitij Sharma","Ayberk Özgür","Philippe Vuadens","Francesco Mondada","Friedhelm Christoph Hummel","Pierre Dillenbourg"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"e1e5778df3538110182a3cf92a3145ad","permalink":"https://CHRI-Lab.github.io/publication/guneysu-2018-iterative/","publishdate":"2019-09-22T10:51:36.52784Z","relpermalink":"/publication/guneysu-2018-iterative/","section":"publication","summary":"Rehabilitation aims to ameliorate deficits in motor control via intensive practice with the affected limb. Current strategies, such as one-on-one therapy done in rehabilitation centers, have limitations such as treatment frequency and intensity, cost and requirement of mobility. Thus, a promising strategy is home-based therapy that includes task specific exercises. However, traditional rehabilitation tasks may frustrate the patient due to their repetitive nature and may result in lack of motivation and poor rehabilitation. In this article, we propose the design and verification of an effective upper extremity rehabilitation game with a tangible robotic platform named Cellulo as a novel solution to these issues. We first describe the process of determining the design rationales to tune speed, accuracy and challenge. Then we detail our iterative participatory design process and test sessions conducted with the help of stroke, brachial plexus and cerebral palsy patients (18 in total) and 7 therapists in 4 different therapy centers. We present the initial quantitative results, which support several aspects of our design rationales and conclude with our future study plans.","tags":null,"title":"Iterative design of an upper limb rehabilitation game with tangible robots","type":"publication"},{"authors":["Wafa Johal","James Kennedy","Vicky Charisi","Hae Won Park","Ginevra Castellano","Pierre Dillenbourg"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"58de15836ceb30e2b0c057832c3a28de","permalink":"https://CHRI-Lab.github.io/publication/johal-2018-robots/","publishdate":"2019-09-22T10:51:36.528186Z","relpermalink":"/publication/johal-2018-robots/","section":"publication","summary":"","tags":null,"title":"Robots for Learning-R4L: Inclusive Learning","type":"publication"},{"authors":["Vicky Charisi","Alyssa M Alcorn","James Kennedy","Wafa Johal","Paul Baxter","Chronis Kynigos"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"5129fb0f55d1f2c09590b25f675e41a9","permalink":"https://CHRI-Lab.github.io/publication/charisi-2018-near/","publishdate":"2019-09-22T10:59:49.226623Z","relpermalink":"/publication/charisi-2018-near/","section":"publication","summary":"Robotics is a multidisciplinary and highly innovative field. Recently, multiple and often minimally connected sub-communities of child-robot interaction have started to emerge, variously focusing on the design issues, engineering, and applications of robotic platforms and toolkits. Despite increasing public interest in robots, including robots for children, child-robot interaction research remains highly fragmented and lacks regular cross-disciplinary venues for discussion and dissemination. This workshop will bring together researchers with diverse scientific backgrounds. It will serve as a venue in which to reflect on the current circumstances in which child-robot research is conducted, articulate emerging and “near future” challenges, and discuss actions and tools with which to meet those challenges and consolidate the field. ","tags":null,"title":"The near future of children's robotics","type":"publication"},{"authors":["Damien Pellier","Carole Adam","Wafa Johal","Humbert Fiorino","Sylvie Pesty"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"eafb3f6f9267a2ea9820663f9fc718af","permalink":"https://CHRI-Lab.github.io/publication/pellier-2018-architecture/","publishdate":"2019-09-22T10:51:36.524394Z","relpermalink":"/publication/pellier-2018-architecture/","section":"publication","summary":"","tags":null,"title":"Une architecture cognitive et affective orientée interaction","type":"publication"},{"authors":["Elmira Yadollahi","Wafa Johal","Ana Paiva","Pierre Dillenbourg"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"0ab6460cce36faf6b48d03abe4cdbbf3","permalink":"https://CHRI-Lab.github.io/publication/yadollahi-2018-deictic/","publishdate":"2019-09-22T10:59:49.22591Z","relpermalink":"/publication/yadollahi-2018-deictic/","section":"publication","summary":"This paper describes research aimed at supporting children's reading practices using a robot designed to interact with children as their reading companion. We use a learning by teaching scenario in which the robot has a similar or lower reading level compared to children, and needs help and extra practice to develop its reading skills. The interaction is structured with robot reading to the child and sometimes making mistakes as the robot is considered to be in the learning phase. Child corrects the robot by giving it instant feedbacks. To understand what kind of behavior can be more constructive to the interaction especially in helping the child, we evaluated the effect of a deictic gesture, namely pointing on the child's ability to find reading mistakes made by the robot. We designed three types of mistakes corresponding to different levels of reading mastery. We tested our system in a within-subject experiment with 16 children. We split children into a high and low reading proficiency even-though they were all beginners. For the high reading proficiency group, we observed that pointing gestures were beneficial for recognizing some types of mistakes that the robot made. For the earlier stage group of readers pointing were helping to find mistakes that were raised upon a mismatch between text and illustrations. However, surprisingly, for this same group of children, the deictic gestures were disturbing in recognizing mismatches between text and meaning.","tags":null,"title":"When deictic gestures in a robot can harm child-robot collaboration","type":"publication"},{"authors":["Ayberk Özgür","Séverin Lemaignan","Wafa Johal","Maria Beltran","Manon Briod","Léa Pereyre","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"a7703455c6ee33172092ec4d8a078878","permalink":"https://CHRI-Lab.github.io/publication/ozgur-2017-cellulo/","publishdate":"2019-09-22T10:51:36.525732Z","relpermalink":"/publication/ozgur-2017-cellulo/","section":"publication","summary":"","tags":null,"title":"Cellulo: Versatile handheld robots for education","type":"publication"},{"authors":["Alexis David Jacq","Wafa Johal","Ana Paiva","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"868574a51b30c39ab419e7a73cf85a0b","permalink":"https://CHRI-Lab.github.io/publication/jacq-2017-expressing/","publishdate":"2019-09-22T10:51:36.527496Z","relpermalink":"/publication/jacq-2017-expressing/","section":"publication","summary":"","tags":null,"title":"Expressing Motivations By Facilitating Other’s Inverse Reinforcement Learning","type":"publication"},{"authors":["Ayberk Özgür","Wafa Johal","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"20bf7cadb088b2c86335061085570055","permalink":"https://CHRI-Lab.github.io/publication/ozgur-2017-haptic/","publishdate":"2019-09-22T10:51:36.526065Z","relpermalink":"/publication/ozgur-2017-haptic/","section":"publication","summary":"The Cellulo robots are small tangible robots that are designed to represent virtual interactive point-like objects that reside on a plane within carefully designed learning activities. In the context of these activities, our robots not only display autonomous motion and act as tangible interfaces, but are also usable as haptic devices in order to exploit, for instance, kinesthetic learning. In this article, we present the design and analysis of the haptic interaction module of the Cellulo robots. We first detail our hardware and controller design that is low-cost and versatile. Then, we describe the task-based experimental procedure to evaluate the robot's haptic abilities. We show that our robot is usable in most of the tested tasks and extract perceptive and manipulative guidelines for the design of haptic elements to be integrated in future learning activities. We conclude with limitations of the system and future work.","tags":null,"title":"Haptic-enabled handheld mobile robots: Design and analysis","type":"publication"},{"authors":["Thibault Asselborn","Wafa Johal","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"446ad379b2355f18f33cad7501345296","permalink":"https://CHRI-Lab.github.io/publication/asselborn-2017-keep/","publishdate":"2019-09-22T10:51:36.526821Z","relpermalink":"/publication/asselborn-2017-keep/","section":"publication","summary":"","tags":null,"title":"Keep on moving! Exploring anthropomorphic effects of motion during idle moments","type":"publication"},{"authors":["Ayberk Özgür","Wafa Johal","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"bc093dcff3b336c7b7a6bf9b43265bde","permalink":"https://CHRI-Lab.github.io/publication/ozgur-2017-windfielddemo/","publishdate":"2020-04-11T21:56:56.818814Z","relpermalink":"/publication/ozgur-2017-windfielddemo/","section":"publication","summary":"","tags":null,"title":"Windfield: demonstrating wind meteorology with handheld haptic robots","type":"publication"},{"authors":["Ayberk Özgür","Wafa Johal","Francesco Mondada","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"f9824efd1b7280392de65bd7b769f838","permalink":"https://CHRI-Lab.github.io/publication/ozgur-2017-windfield/","publishdate":"2019-09-22T10:51:36.525411Z","relpermalink":"/publication/ozgur-2017-windfield/","section":"publication","summary":"","tags":null,"title":"Windfield: learning wind meteorology with handheld haptic robots","type":"publication"},{"authors":["Wafa Johal","Paul Vogt","James Kennedy","Mirjam de Haas","Ana Paiva","Ginevra Castellano","Sandra Okita","Fumihide Tanaka","Tony Belpaeme","Pierre Dillenbourg"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"d0685337fc9c356852644e315048cc16","permalink":"https://CHRI-Lab.github.io/publication/johal-2017-workshop/","publishdate":"2019-09-22T10:51:36.526492Z","relpermalink":"/publication/johal-2017-workshop/","section":"publication","summary":"While robots have been popular as a tool for STEM teaching, the use of robots in other learning scenarios is novel. The field of HRI has started to report on how to make effective robots usable in educational contexts. However, many challenges remain. For instance, which interaction strategies aid learning, and which hamper learning? How can we deal with the current technical limitations of robots? Answering these and other questions requires a multidisciplinary effort, including contributions from pedagogy, developmental psychology, (computational) linguistics, artificial intelligence and HRI, among others. This abstract provides a brief overview of the current state-of-the-art in social robots designed for learning and describes the aims of the Robots for Learning (R4L) workshop in bringing together a multidisciplinary audience for furthering the development of market-ready educational robots.","tags":null,"title":"Workshop on Robots for Learning: R4L","type":"publication"},{"authors":null,"categories":null,"content":"In the field of human-robot interaction (HRI) as in many other technical fields, an innumerable number of different metrics to analyze the interaction can be found in as many studies. Even-though many of these metrics are not\ncomparable between studies, we observe that the research community in HRI, but also in many other research domain, is starting to seek for reproducibility [10]; a consensus begins to appear concerning common measures that can be used across a wide range of studies. In social HRI, the evaluation of the quality of an interaction is complex because it is very task dependent. However, certain metrics such as engagement seem to well reflect the quality of interactions between a robot agent and the human.\nOne aspect of acceptability of a robot is the home environment in which it is to be able to perceive when it will be solicited. The goal for the robot is to not disturb the user and to be able to predict that it will be solicited. This is something we do all the time as humans (we can see a street vendor approaching us and we know they will talk to us). The process for us human relies on proxemics (speed and angle of approach) but not only.\nIn my work on modeling engagement [11, 12], I used multi-modal data to train a SVM to detect engagement. We\ncollected data from various sensors embedded in the Kompai robot and reduced the number of crucial features from 32 to 7. Interestingly shoulder orientation and position of the face in the image are among these crucial features. If we transpose these features to what humans do, these features seem coherent with behavioral analysis. The previous model aimed to predict the engagement, but once the user is engaged, it is important to evaluate its attitude and mood. Using the COST and HAART dataset, we trained a model to detect social touch. Together with colleagues, we won the Social Touch Challenge at ICMI 2015 improving the gestures recognition from 60% accuracy to 70% training a Random Forest Model. [13]\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"58649cc8a6ece3333314200594b29e71","permalink":"https://CHRI-Lab.github.io/project/engagement/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/engagement/","section":"project","summary":"Understanding human actions, intentions is key","tags":["affective computing","cowriter"],"title":"Behavioural Analysis in HRI","type":"project"},{"authors":null,"categories":null,"content":"I have been developing a model of the so called behavioral styles. These styles act like a filter over communicative gestures to transpose a way of performing an action. We used multiple platforms (Nao – humanoid, and Reeti – facial expression) to test this rendering on facial and bodily communication [14]. We showed that these styles were perceptible and could influence the attitude of the child interacting with the robot[15, 16]. More recently, we showed that idle movements (movements that have no communicative intention) when displayed by a humanoid robot increases the anthropomorphic perception of the robot by the user [17]. These findings help in designing more natural interaction with humanoid robots, making them more acceptable and socially intelligent. Research Perspectives: We will be continuing research in this area working within the ANIMATAS EU project (starting in January 2018) on synchrony and how alignment can keep learners engaged in a collaborative task with a robot.\n     ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2565fa501f241664cf0ff7bc073c37f8","permalink":"https://CHRI-Lab.github.io/project/stylebot/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/stylebot/","section":"project","summary":"Generating context aware gestures","tags":["styles","moca"],"title":"Behavioural Styles","type":"project"},{"authors":null,"categories":null,"content":"With the Cellulo project [6], a part of the Transversal Educational Activities of the NCCR Robotics, we introduced a new robotic platform which is small and easily deployable. A dotted pattern printed on regular paper enables the Cellulo robots with absolute localization with a precision of 270 microns [7]. The robots also have a new locomotion system with a drive relying on a permanent magnet to actuate coated metal balls [8]. This new drive design allows backdrivability; i.e. it allows the robot to move and to be moved without damaging it. With this system, we also implemented a haptic feedback modality, allowing the user to feel forces when grasping the robot [9]. The robots are connected via Bluetooth to a master (PC or tablet) that handles the logical and computation of the activity. The onboard PCB of the robots only allows for proceeding the localization (image capture and decoding of the pattern) and the control of the three wheels actuation. During two years, we developed several learning activities using the robots. The Figure for example shows the Feel the Wind activity, in which the learners were taught that the wind was formed by air moving from with high to low pressure points.\n  In the Cellulo project, we also started to explore the use of haptic feedback for learners. Haptic feedback enables us to render forces, but also borders, angles, or points. We developed a series of haptic capabilities and small interaction tasks that can be included in learning activities to inform the learner [9]. We tested the haptic feedback with children for instance in the symmetry activity, in which the child is able to formulate hypothesis on the placement of the symmetrical shape and to verify their claims by feel haptically the shape on paper (left Figure). We also tested with some pilot with visually impaired children who were able to explore a map of their classroom using the Cellulo robots. Research Perspectives for Tangible Swarm Interaction and Haptic for Learners: We are now exploring the dynamics of the group of learners in manipulating the robots. The collaboration among learner is not always optimal, and a challenge would be to use the swarm robots to analyses and regulate the collaboration among learners. As these shared resource can be intelligent agents, they could rearrange themselves according to the collaborative state of the group.\n    ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"fcd133806e4dabbfec24bdc44e8750ef","permalink":"https://CHRI-Lab.github.io/project/cellulo/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/cellulo/","section":"project","summary":"Tangible Haptic-Enabled Swarm for Learners","tags":["cellulo","tangible robot","swarm","haptic"],"title":"Cellulo","type":"project"},{"authors":null,"categories":null,"content":"Kazakhstan has recently adopted a state program for the development and functioning of languages for 2011-2020. This new trilingual education policy aimed at development among the Kazakhs of fluency in three languages: Kazakh, Russian and English. Additionally, a recent decision on the transfer of Kazakh language from Cyrillic into the Latin alphabet was approved by the Kazakh authorities in October 2017 [1] While there are clear reasons for these reforms, there are numerous risks facing the transfer, including risks to increase inequalities in education services (e.g. preference for schools with teachers better trained in English or for Russian-speaking schools), to cause illiteracy in adults in their native language, and to cause disinterest and lack of motivation to write and read Kazakh among Kazakh and non-Kazakh children and adults. Beyond familiarity with local conditions, assessing and managing these risks requires understanding the effects they can have on a variety of those affected including children, teachers, and adults, who on the one hand are comfortably able to read the basic Latin script (e.g. English), but on the other, will not recognize crucial distinctions projected onto familiar graphemes.\nSince 2014, the CoWriter project has explored how robotic technologies can help children with the training of handwriting via an original paradigm known as learning by teaching (LbT) [2,3,4]. Since the children act as the teachers who help the robot to learn handwriting, the children practice their handwriting even without noticing it and stay committed to the success of the robot via the Protege effect. Previous research have shown the motivational aspect of the LbT with a robot for handwriting [3]. However, a long-term effect on learning handwriting skill has still to be demonstrated. Nevertheless, we believe that the CoWriter activity has the required innovative aspect to it and, hence, it can boost the children’s self-esteem and motivation to learn the Latin-based Kazakh alphabet and its handwriting.\nThe proposed collaboration aims to benefit from the new Language Planning in Kazakhstan in order to address 1) challenges of training and motivating children to learn and use a new alphabet, 2) cross-cultural differences between Switzerland and Kazakhstan in the context of robots for learning, 3) to develop a novel approach for training teachers and the adult population e.g. a CoWriter’s smartphone version.\nWith this Seed Funding Grant, we propose to initiate this collaboration via two main phases: 1) data collection required for adapting Kazakh language in both alphabets into the CoWriter and 2) implementation of the \u0026ldquo;CoWriting Kazakh\u0026rdquo; child-robot interaction activity.\nThe data collection phase includes collecting children’s handwriting data using a Wacom tablet. This data is required for training the CoWriter’s learning algorithm. Then we propose to adapt the CoWriter to the Kazakh alphabet and to explore whether it is the best approach to start the switch from the first graders in comparison to children who already know some English as the Latin alphabet might be easier for them. The implementation phase also includes the CoWriter’s deployment by conducting a series of experiments with children and teachers investigating cross-cultural differences in the classrooms.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"1f74630fb648c43f4c4d2fe7a564ff54","permalink":"https://CHRI-Lab.github.io/project/cowriting_kazakh/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/cowriting_kazakh/","section":"project","summary":"A social robot to help the transition from Cyrillic to Latin alphabet in Kazakhstan","tags":["cowriter","handwriting","cowriting_kazakh"],"title":"CoWriting Kazakh","type":"project"},{"authors":null,"categories":null,"content":"Many children struggle to find their voice in social situations. They may be shy, suffer from social anxiety, not speak the native language as their first language, be new to a culture (migrants), or have a special need involving communication impairments (e.g., autism, speech, and hearing) [1]. Their voices often go unheard as they find it hard to contribute to conversations and group work, in both face to face and online settings, which puts them at risk for poor socio-emotional and achievement outcomes.\nVarious interventions have been proposed in an effort to help these children integrate better into social settings, including ice-breaking activities, support groups, [2][3]. Another promising alternative is to teach children how to interpret and use humor effectively to facilitate social interaction [4]. Having a sense of humour might help children to reduce stress [5] and ease social interactions, enabling them to overcome their social inhibitions and make friends more easily. Also, humour could promotes children’s language, literacy, and inference skills, since understanding jokes require a mastery of language ambiguity, and comprehension [6]. One manner to use humor effectively is by ‘joke telling’. A joke is the shortest clearly-structured conversation one can have with one or more peers. Telling jokes can help children improve their confidence, socialization, [4], literacy [8], and cultural learning [7]. For example, a child could start a ‘joke’ that implies a question. He/she knows the answer, but not his/her peers. Then, he/she has the advantage of knowing what the answer is. This helps him/her to gain confidence. In terms of literacy, telling a joke requires the reinterpretation of the syntax underlying the question, then, understanding the ambiguity in a joke might help them in the comprehension of language [8]. Regarding socialization, this dynamic allows children to take part in role-taking, and understanding what other people needs to know to get it. Also, it can create a positive environment where children can laugh together and share their understanding. In terms of learning, a jokes encapsulate cultural assumptions, for example, differences between pronunciation, a dual meaning of a word, and the structural and ambiguity of sentences [7]. The question this raises is: how can we help children from all walks of life learn the art of telling jokes and importantly feel comfortable, and in doing so, increase their confidence, social skills and even literacy? To help children develop the ability to tell jokes with their peers, we propose leveraging a new interactive technology – voice assistant robots (e.g., Alexa), which have become increasingly affordable and commonplace in homes. These robots, which are portable and have lifelike voices, can model how to tell a joke and using voice recognition software, respond to the jokes that children tell. Children thus have unlimited opportunities for scaffolded practice and can receive timely feedback in a non-threatening environment before telling their jokes to family and friends. In addition to the accessibility and human-like verbal responses of voice assistant robots, robots can be customized to tell jokes that are appropriate for each child’s age, language, and culture. The goal of our project therefore is to enable children to improve their ‘joke telling’ skills by learning the intonation, timing, through interacting with a virtual assistants such as Alexa. Our research project will explore the feasibility of coding Alexa and other voice assistants to be able to respond and engage with children telling a diversity of jokes. Several aspects need to be taken into account when delivering a joke that children can imitate and practice with. We will look into how the timing and intonation will need to be evaluated from the audio recording in order to provide an adaptive feedback to the child training to deliver jokes. This feedback could be from both sides, the voice assistant robot helping the child in how telling jokes, or the child helping the assistant to improve how telling jokes. The final goal is, once the child has learnt how to tell jokes, to understand when and how will he/she be able to share the joke with their peers, and how long might this process take and how would we be able to tell they have become more confident. Finally, an engaging joke-telling learning scenario will need to be designed in order to train children for a long-term impact. Our project will also consider how this might materialize. In the future, virtual assistants will be able to be designed to give encouraging feedback to the child as they interact with it. This could be verbal, visual or both. This feedback could encourage children for finding their voice. Based on the initial findings from the pilot project we would like to develop a more extensive project proposal with the goal of investigating how to develop new interventions using the next generation of interactive robots, such as Olly, whose developers, we are also collaborating with (https://heyolly.com/), that have more scope for providing customized feedback can be used.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"45f8841eb64aea3141bafd94c1162fff","permalink":"https://CHRI-Lab.github.io/project/fyv/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/fyv/","section":"project","summary":"Helping children's language understanding through joke telling with home assistant","tags":["affective computing","learning companion","home assistant"],"title":"Find Your Voice","type":"project"},{"authors":null,"categories":null,"content":"With the increasing number of engineering jobs, politics have more recently turned towards introduction of engineering subjects in early stages of curricula. More and more countries have started to introduce programming (and even robotics) to young children. However, this constitutes a real challenge for the teaching professionals, who are not trained for that, and are often skeptical to use new technologies in classrooms. Hence, the challenge is to introduce robots as tools to be used not only in programming or robotics based courses, but in the whole curricula in order to make them attractive to teachers as a pedagogical tool. ”Educational robotics” does not really constitute a research community per say. On one hand, there are scholars working on ”how to teach robotics” using robotics platforms such as Thymio and Mindstorms. Some scholars perform research in this domain (for example, measuring which learning activities produce faster learning), but their numbers are scarce, they typically meet in half-day workshops before ed’tech conferences (CSCL, AI\u0026amp;Ed, ….). On the other hand, one finds scholars who mainly do research on HRI and consider education as an interesting place for testing child-robot interactions. I launched in 2016 a series of workshops to build a Robots for Learning (R4L) research community. The first event was a workshop, along with the RoMan 2016 conference. The event gathered 30 participants from all around the world. The second event was a workshop along the HRI 2017 conference in Vienna, Austria, with 60 participants and 10 presentations. This fall, we hosted the first stand alone event, in Switzerland, for which we invited main actors of research in HRI for Learning. A new workshop is planned for HRI 2018. These workshops aim to mix scientists from field of robotics with those from digital education and learning technologies (See http://robots4learning.wordpress.com).\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e4802fe1f529e6392518b0e58822db5d","permalink":"https://CHRI-Lab.github.io/project/robots4learning/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/robots4learning/","section":"project","summary":"Understading the value of robots for leaners","tags":["r4l"],"title":"Robots for Learning (R4L)","type":"project"},{"authors":null,"categories":null,"content":"Grants for postdocs\nGrans for visitors: french unsw others\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"98e785ba537fca98e25a25185efab61f","permalink":"https://CHRI-Lab.github.io/prospective/visitors/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/prospective/visitors/","section":"prospective","summary":"Grants for postdocs\nGrans for visitors: french unsw others","tags":null,"title":"Visitors and PostDoc","type":"prospective"},{"authors":["Wafa Johal","Alexis Jacq","Ana Paiva","Pierre Dillenbourg"],"categories":null,"content":"  ","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"4a69452696c10f2aa57a5d50b54cdb76","permalink":"https://CHRI-Lab.github.io/publication/johal-2016-child/","publishdate":"2019-09-22T10:51:36.524069Z","relpermalink":"/publication/johal-2016-child/","section":"publication","summary":"In  this  paper,  we  present  an  experiment  in  the context of a child-robot interaction where we study the influence of  the  child-robot  spatial  arrangement  on  the  child’s  focus  of attention and the perception of the robot’s performance. In the “Co-Writer learning by teaching” activity, the child teaches a Nao robot how to handwrite. Usually only face-to-face spatial arrangements are tested in educational child robot interactions, but  we  explored  two  spatial  conditions  from  Kendon’s  F-formation,  the  side-by-side  and  the  face-to-face  formations  in a  within  subject  experiment.  We  estimated  the  gaze  behavior of  the  child  and  their  consistency  in  grading  the  robot  with regard  to  the  robot’s  progress  in  writing.  Even-though  the demonstrations provided by children were not different between the two conditions (i.e. the robot’s learning didn’t differ), the results showed that in the side-by-side condition children tended to  be  more  indulgent  with  the  robot’s  mistakes  and  to  give it  better  feedback.  These  results  highlight  the  influence  of experimental choices in child-robot interaction","tags":null,"title":"Child-robot spatial arrangement in a learning by teaching activity","type":"publication"},{"authors":["Alexis Jacq","Wafa Johal","Pierre Dillenbourg","Ana Paiva"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"5db07a2a81b91de3a83f41a1cc4e9a47","permalink":"https://CHRI-Lab.github.io/publication/jacq-2016-cognitive/","publishdate":"2019-09-22T10:51:36.523763Z","relpermalink":"/publication/jacq-2016-cognitive/","section":"publication","summary":"","tags":null,"title":"Cognitive architecture for mutual modelling","type":"publication"},{"authors":["Ayberk Özgür","Wafa Johal","Pierre Dillenbourg"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"e36af435970f54d5ee280f35968e53e4","permalink":"https://CHRI-Lab.github.io/publication/ozgur-2016-permanent/","publishdate":"2019-09-22T10:51:36.523414Z","relpermalink":"/publication/ozgur-2016-permanent/","section":"publication","summary":"","tags":null,"title":"Permanent magnet-assisted omnidirectional ball drive","type":"publication"},{"authors":["Francesco Mondada","Evgeniia Bonnet","Shaniel Davrajh","Wafa Johal","Riaan Stopforth"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"7142bad334ef2027986481c415b041b1","permalink":"https://CHRI-Lab.github.io/publication/mondada-2016-r-2-t-2/","publishdate":"2019-09-22T10:51:36.525073Z","relpermalink":"/publication/mondada-2016-r-2-t-2/","section":"publication","summary":"","tags":null,"title":"R2T2: Robotics to integrate educational efforts in South Africa and Europe","type":"publication"},{"authors":["Carole Adam","Wafa Johal","Damien Pellier","Humbert Fiorino","Sylvie Pesty"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"7e174f5b06ad4e07d7e887492dc444cf","permalink":"https://CHRI-Lab.github.io/publication/adam-2016-social/","publishdate":"2019-09-22T10:51:36.524708Z","relpermalink":"/publication/adam-2016-social/","section":"publication","summary":"","tags":null,"title":"Social human-robot interaction: A new cognitive and affective interaction-oriented architecture","type":"publication"},{"authors":["Dominique Vaufreydaz","Wafa Johal","Claudine Combe"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"c5350178a8c0355d12bae0757a84dd78","permalink":"https://CHRI-Lab.github.io/publication/vaufreydaz-2016-starting/","publishdate":"2019-09-22T10:51:36.52114Z","relpermalink":"/publication/vaufreydaz-2016-starting/","section":"publication","summary":"Recognition of intentions is a subconscious cognitive process vital to human communication. This skill enables anticipation and increases the quality of interactions between humans. Within the context of engagement, non-verbal signals are used to communicate the intention of starting the interaction with a partner. In this paper, we investigated methods to detect these signals in order to allow a robot to know when it is about to be addressed. Originality of our approach resides in taking inspiration from social and cognitive sciences to perform our perception task. We investigate meaningful features, i.e. human readable features, and elicit which of these are important for recognizing someone’s intention of starting an interaction. Classically, spatial information like the human position and speed, the human–robot distance are used to detect the engagement. Our approach integrates multimodal features gathered using a companion robot equipped with a Kinect. The evaluation on our corpus collected in spontaneous conditions highlights its robustness and validates the use of such a technique in a real environment. Experimental validation shows that multimodal features set gives better precision and recall than using only spatial and speed features. We also demonstrate that 7 selected features are sufficient to provide a good starting engagement detection score. In our last investigation, we show that among our full 99 features set, the space reduction is not a solved task. This result opens new researches perspectives on multimodal engagement detection.","tags":null,"title":"Starting engagement detection towards a companion robot using multimodal features","type":"publication"},{"authors":["Wafa Johal","Damien Pellier","Carole Adam","Humbert Fiorino","Sylvie Pesty"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"8b77c395ebf8d06e51fc50f17e1988c3","permalink":"https://CHRI-Lab.github.io/publication/johal-2015-cognitive/","publishdate":"2019-09-22T10:51:36.522473Z","relpermalink":"/publication/johal-2015-cognitive/","section":"publication","summary":"","tags":null,"title":"A cognitive and affective architecture for social human-robot interaction","type":"publication"},{"authors":["Wafa Benkaouar Johal"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"815db4b8be8d9f59f4aaf552407df2a9","permalink":"https://CHRI-Lab.github.io/publication/johal-2015-companion/","publishdate":"2019-09-22T10:51:36.527156Z","relpermalink":"/publication/johal-2015-companion/","section":"publication","summary":"","tags":null,"title":"Companion Robots Behaving with Style: Towards Plasticity in Social Human-Robot Interaction","type":"publication"},{"authors":["Wafa Johal","Gaëlle Calvary","Sylvie Pesty"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"d805c068954b4ff05daabdf1774ccb39","permalink":"https://CHRI-Lab.github.io/publication/johal-2015-non/","publishdate":"2019-09-22T10:51:36.523095Z","relpermalink":"/publication/johal-2015-non/","section":"publication","summary":"","tags":null,"title":"Non-verbal Signals in HRI: Interference in Human Perception","type":"publication"},{"authors":["Wafa Johal"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"8704a63253d14622dbe48d75098551ef","permalink":"https://CHRI-Lab.github.io/publication/johal-2015-robots/","publishdate":"2019-09-22T10:51:36.522159Z","relpermalink":"/publication/johal-2015-robots/","section":"publication","summary":"","tags":null,"title":"Robots Interacting with Style","type":"publication"},{"authors":["Viet-Cuong Ta","Wafa Johal","Maxime Portaz","Eric Castelli","Dominique Vaufreydaz"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"f9523271ca0ab1ffc94f95f895a52ef2","permalink":"https://CHRI-Lab.github.io/publication/ta-2015-grenoble/","publishdate":"2019-09-22T10:51:36.52279Z","relpermalink":"/publication/ta-2015-grenoble/","section":"publication","summary":"New technologies and especially robotics is going towards more natural user interfaces. Works have been done in different modality of interaction such as sight (visual computing), and audio (speech and audio recognition) but some other modalities are still less researched. The touch modality is one of the less studied in HRI but could be valuable for naturalistic interaction. However touch signals can vary in semantics. It is therefore necessary to be able to recognize touch gestures in order to make human-robot interaction even more natural. We propose a method to recognize touch gestures. This method was developed on the CoST corpus and then directly applied on the HAART dataset as a participation of the Social Touch Challenge at ICMI 2015. Our touch gesture recognition process is detailed in this article to make it reproducible by other research teams. Besides features set description, we manually filtered the training corpus to produce 2 datasets. For the challenge, we submitted 6 different systems. A Support Vector Machine and a Random Forest classifiers for the HAART dataset. For the CoST dataset, the same classifiers are tested in two conditions: using all or filtered training datasets. As reported by organizers, our systems have the best correct rate in this year's challenge (70.91% on HAART, 61.34% on CoST). Our performances are slightly better that other participants but stay under previous reported state-of-the-art results.","tags":null,"title":"The Grenoble system for the social touch challenge at ICMI 2015","type":"publication"},{"authors":["Wafa Johal","Gaelle Calvary","Sylvie Pesty"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"00da623763bdef5e3c44a84531e7a714","permalink":"https://CHRI-Lab.github.io/publication/johal-2014-robot/","publishdate":"2019-09-22T10:51:36.520174Z","relpermalink":"/publication/johal-2014-robot/","section":"publication","summary":"","tags":null,"title":"A Robot with Style, because you are Worth it!","type":"publication"},{"authors":["Wafa Johal","Carole Adam","Humbert Fiorino","Sylvie Pesty","Céline Jost","Dominique Duhaut"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"5275f877fb800832d7e6b6d1c3c0a112","permalink":"https://CHRI-Lab.github.io/publication/johal-2014-acceptability/","publishdate":"2019-09-22T10:51:36.521482Z","relpermalink":"/publication/johal-2014-acceptability/","section":"publication","summary":"","tags":null,"title":"Acceptability of a companion robot for children in daily life situations","type":"publication"},{"authors":["Wafa Johal","Sylvie Pesty","Gaëlle Calvary"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"3449b6f6eaab0c4731831c6e2c871b3c","permalink":"https://CHRI-Lab.github.io/publication/johal-2014-styles/","publishdate":"2019-09-22T10:51:36.521838Z","relpermalink":"/publication/johal-2014-styles/","section":"publication","summary":"","tags":null,"title":"Des Styles pour une Personnalisation de l'Interaction Homme-Robot","type":"publication"},{"authors":["Wafa Johal","Sylvie Pesty","Gaëlle Calvary"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"90b5a2537feadc3dac7866a597f0f0a3","permalink":"https://CHRI-Lab.github.io/publication/johal-2014-expressing/","publishdate":"2019-09-22T10:51:36.5205Z","relpermalink":"/publication/johal-2014-expressing/","section":"publication","summary":"","tags":null,"title":"Expressing Parenting Styles with Companion Robots","type":"publication"},{"authors":["Wafa Johal","Sylvie Pesty","Gaelle Calvary"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"ee11b2e0b1d7eface3aeda38af0392f8","permalink":"https://CHRI-Lab.github.io/publication/johal-2014-towards/","publishdate":"2019-09-22T10:51:36.520816Z","relpermalink":"/publication/johal-2014-towards/","section":"publication","summary":"","tags":null,"title":"Towards companion robots behaving with style","type":"publication"},{"authors":["Carole Adam","Wafa Johal","Ilef Ben Farhat","Céline Jost","Humbert Fiorono","Sylvie Pesty","Dominique Duhaut"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"bdabfe673ea5442c37aa6672017dcd59","permalink":"https://CHRI-Lab.github.io/publication/adam-2013-acceptabilite/","publishdate":"2019-09-22T10:51:36.519868Z","relpermalink":"/publication/adam-2013-acceptabilite/","section":"publication","summary":"","tags":null,"title":"Acceptabilité d'un robot compagnon dans des situations de la vie quotidienne.","type":"publication"},{"authors":["Wafa Johal","Julie Dugdale","Sylvie Pesty"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"d5abe417be4d6de04bf78e4927c52a1d","permalink":"https://CHRI-Lab.github.io/publication/johal-2013-modelling/","publishdate":"2019-09-22T10:51:36.519511Z","relpermalink":"/publication/johal-2013-modelling/","section":"publication","summary":"","tags":null,"title":"Modelling interactions in a mixed agent world.","type":"publication"},{"authors":["Wafa Benkaouar Johal"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637921694,"objectID":"4214a59d4d950eef3d82db4b16a3fa23","permalink":"https://CHRI-Lab.github.io/publication/benkaouar-2012/","publishdate":"2021-11-26T10:14:53.889419Z","relpermalink":"/publication/benkaouar-2012/","section":"publication","summary":"","tags":[],"title":"Detection of non-verbal communication cues using multi-modal sensors : engagement detection ","type":"publication"},{"authors":["Wafa Benkaouar","Dominique Vaufreydaz"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"bd7f2b732ade4c8dbe767ed7db29a545","permalink":"https://CHRI-Lab.github.io/publication/benkaouar-2012-multi/","publishdate":"2019-09-22T10:51:36.51914Z","relpermalink":"/publication/benkaouar-2012-multi/","section":"publication","summary":"Recognition of intentions is an unconscious cognitive process vital to human communication. This skill enables anticipation and increases interactive exchanges quality between humans. Within the context of engagement, i.e. intention for interaction, non-verbal signals are used to communicate this intention to the partner. In this paper, we investigated methods to detect these signals in order to allow a robot to know when it is about to be addressed. Classically, the human position and speed, the human-robot distance are used to detect the engagement. Our hypothesis is that this method is not enough in a context of a home environment. The chosen approach integrates multimodal features gathered using a robot enhanced with a Kinect. The evaluation of this new method of detection on our corpus collected in spontaneous conditions highlights its robustness and validates use of such technique in real environment. Experimental validation shows that the use of multimodal sensors gives better precision and recall than the detector using only spatial and speed features. We also demonstrate that 7 multimodal features are sufﬁcient to provide a good engagement detection score.","tags":null,"title":"Multi-sensors engagement detection with a robot companion in a home environment","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://CHRI-Lab.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://CHRI-Lab.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"One of my interests is to make robots able to autonomously sustain interactions with users, and in order to do so, they have to be able to reason about the users’ and their environment. During my PhD, I have been working on a Cognitive Architecture able to reason on Emotion, named CAIO (Cognitive and Affective Interaction-Oriented) Architecture [3, 4]. This architecture, based on symbolic reasoning, showed promising results in modeling cognitive processes and specifically allowing decision making based on emotions. As shown in the figure, this architecture works as a two loops process, similar to the Dual-Process Theory - a deliberative loop generating intentions and a sensorimotor loop handling reflexes.\nMore recently, we have been working on second order reasoning in the context of the CoWriter project [5]. In the CoWriter project, the child’s teaches a Nao robot how to write. We use the learning by teaching paradigm to enhance motivation and engagement. In a collaborative learning task between a robot and a child, the idea is to model the child’s understanding and the child’s believes of the understanding of the co-learner robot. This way the robot could detect misunderstandings in view to correct them; or the robot could even create misunderstandings to enhance learning (by fostering questioning). Since my arrival on the CoWriter project, we initiated a project on diagnosis of dysgraphia using data collected via a graphic tablet (Wacom). Our first results using RNN are very promising (a patent and a journal paper have been submitted). This work will later on be integrated in the CoWriter handwriting activities to adapt the learning path according to the diagnosis and the learner’s handwriting difficulties.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"56381d78247340c21f180f4c2e39fcfa","permalink":"https://CHRI-Lab.github.io/project/caio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/caio/","section":"project","summary":"Cognitive and Affective Interaction-Oriented","tags":["reasoning","caio"],"title":"CAIO","type":"project"},{"authors":null,"categories":null,"content":"Call for Participants We are currently conducting a research project on whether we can determine handwriting legibility solely from a person\u0026rsquo;s handwriting without the need for specialised tests. Right now, we are in the process of collecting digital handwriting using a tablet-based web application.\nSo if you have a tablet with a digital stylus and would like to contribute your handwriting to this project, please click on the following link -\u0026gt; https://digital-pen-746a6.web.app/\nThe task will take less than 5 minutes and all responses are totally anonymous.\nThanks in advance for your support!!!\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"75f2bb400e66b79e3530b01e1d6f7802","permalink":"https://CHRI-Lab.github.io/project/handwriting/digital_pen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/handwriting/digital_pen/","section":"project","summary":"Students handwriting analysis project to help tem be ready for written exams","tags":["handwriting","machine-learning","-cse-honours"],"title":"Digital Pen","type":"project"},{"authors":null,"categories":null,"content":"Context Handwriting is one of the most important motor skill we learn as human. Children with handwriting difficulties can find their academic impacted and often succeed less at school. It is even more true for people having to learn different handwriting scripts (i.e. latin, chinese, arabic)\nIn this project we propose to explore new methods to assess and train people\u0026rsquo;s handwriting in a multiscript handwriting application. The project will aim to develop a new engaging handwriting analysis tool and integrate the analysis in agamified application. The backend of the application will be performing the analsysi of handwirting through a library taking into account various features of the handwriting logs (i.e. pen pressure, tilt, speed).\nGoals \u0026amp; Milestones During this project, the student will:\n Develop a library able to analyse strokes and handwriting (backend) Develop a JS app able to record handwriting data Integrate gamification into the app to build a learning game Evaluate the implemented application with end-users evaluating both usability and performances (learning outcomes)  Topics Handwriting, JS, Algoritms,\nPrerequisites  Skills: JS, Python, Git.  References See Collection https://wafa.johal.org/tags/handwriting/\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4a1c79003c419bd1d1e46ac58e8cc7ea","permalink":"https://CHRI-Lab.github.io/prospective/undergrad/e-pen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/e-pen/","section":"prospective","summary":"Context Handwriting is one of the most important motor skill we learn as human. Children with handwriting difficulties can find their academic impacted and often succeed less at school. It is even more true for people having to learn different handwriting scripts (i.e. latin, chinese, arabic)\nIn this project we propose to explore new methods to assess and train people\u0026rsquo;s handwriting in a multiscript handwriting application. The project will aim to develop a new engaging handwriting analysis tool and integrate the analysis in agamified application. The backend of the application will be performing the analsysi of handwirting through a library taking into account various features of the handwriting logs (i.e. pen pressure, tilt, speed).\nGoals \u0026amp; Milestones During this project, the student will:\n Develop a library able to analyse strokes and handwriting (backend) Develop a JS app able to record handwriting data Integrate gamification into the app to build a learning game Evaluate the implemented application with end-users evaluating both usability and performances (learning outcomes)  Topics Handwriting, JS, Algoritms,\nPrerequisites  Skills: JS, Python, Git.  References See Collection https://wafa.johal.org/tags/handwriting/","tags":["handwriting","co-writer","cse-honours","active"],"title":"E-Pen","type":"prospective"},{"authors":null,"categories":null,"content":"Context Many children struggle to find their voice in social situations. They may be shy, suffer from social anxiety, be new to a culture (migrants) or have impairments in communication due to atypical development (e.g., autism spectrum disorder, speech or hearing disorders).\nThe voices of these children often go unheard, as they find it hard to contribute to a conversation.\nThe Find your Voice (FyV, http://wafa.johal.org/project/fyv/) project was initiated to investigate how joke telling could help children to speak up and gain confidence. We are also interested in story telling and general conversation. Improvements in communication can have a significant impact in confidence, and help children:\n reduce stress improve self-confidence ease social interactions make friends more easily improving literacy and language  To help children develop the ability to communicate, tell jokes or stories to their peers, we propose leveraging social robots (e.g. NAO) and voice assistants (e.g., Alexa, Olly and Google Home) to:\n Model how to tell jokes/stories and respond to other children during conversations . Practice joke/story telling with a ‘friendly’ and ‘non-judgmental’ audience. Practice turn taking during conversation. TLearn jokes, stories and interesting facts to tell other children.  The overall goals of the project are:\n To enable children to improve their social communication skills by learning intonation and timing, through interacting with voice assistants To learn to how to perform in front of peers and family To make children more confident in social situations  The FyV project involves partners in London and California.\nGoals \u0026amp; Milestones At UNSW, our main goal will be to develop a ‘Learning by Teaching’ application using a robot or voice assistant. This application will allow the user to teach a virtual agent (robot or voice assistant) a joke/story. As the agent learns by demonstration, the user can practice and refine how the story/joke is told until the voice assistant (and the child) is able to tell the joke/story in a satisfactory way.\n Design the Learning Scenario Explore TTS software for speech conversion Implement a new Alexa Skill Run a Pilot demonstrating the learning of joke/story telling features (e.g. pauses and intonations)  Topics Voice Assistant, Machine Learning, HCI\nPrerequisites  Skills: Python or C++. Git.  References  https://github.com/soobinseo/Transformer-TTS https://github.com/barronalex/Tacotron https://developer.amazon.com/en-US/alexa/alexa-skills-kit  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3fccfa65c20178130e3b95f7972f12e7","permalink":"https://CHRI-Lab.github.io/prospective/undergrad/fyv_2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/fyv_2020/","section":"prospective","summary":"Context Many children struggle to find their voice in social situations. They may be shy, suffer from social anxiety, be new to a culture (migrants) or have impairments in communication due to atypical development (e.g., autism spectrum disorder, speech or hearing disorders).\nThe voices of these children often go unheard, as they find it hard to contribute to a conversation.\nThe Find your Voice (FyV, http://wafa.johal.org/project/fyv/) project was initiated to investigate how joke telling could help children to speak up and gain confidence. We are also interested in story telling and general conversation. Improvements in communication can have a significant impact in confidence, and help children:\n reduce stress improve self-confidence ease social interactions make friends more easily improving literacy and language  To help children develop the ability to communicate, tell jokes or stories to their peers, we propose leveraging social robots (e.g. NAO) and voice assistants (e.g., Alexa, Olly and Google Home) to:\n Model how to tell jokes/stories and respond to other children during conversations . Practice joke/story telling with a ‘friendly’ and ‘non-judgmental’ audience. Practice turn taking during conversation. TLearn jokes, stories and interesting facts to tell other children.  The overall goals of the project are:\n To enable children to improve their social communication skills by learning intonation and timing, through interacting with voice assistants To learn to how to perform in front of peers and family To make children more confident in social situations  The FyV project involves partners in London and California.\nGoals \u0026amp; Milestones At UNSW, our main goal will be to develop a ‘Learning by Teaching’ application using a robot or voice assistant. This application will allow the user to teach a virtual agent (robot or voice assistant) a joke/story. As the agent learns by demonstration, the user can practice and refine how the story/joke is told until the voice assistant (and the child) is able to tell the joke/story in a satisfactory way.","tags":["voice-assistant","cse-honours"],"title":"Find Your Voice: Use of Voice Assistant for Learning","type":"prospective"},{"authors":null,"categories":null,"content":"Context Action Recognition is curcial for robots to perfoma around humans. Indeed, robot need to asses human action and intentions in order to assist them in everyday life tasks and to collaborative efficiently.\nThe field of action recognition has aimed to use typical sensors found on robots to recognize agnts, objects and actions they are performing. Typical approach is to record a dataset of various action and label them. But often theses actions are not natural and it can be difficult to represent the variety of ways to perform actions with a lab built dataset. In this project we propose to use audio desription movies to label actions. AD movies integrate a form of narration to allow virually impaired veiwers to undertsnad the visual element sowed on screen. This information often deals with action actually depicted on the scene.\nGoals \u0026amp; Milestones During this project, the student will:\n Develop a pipeline to collect and crop clip of AD movies for at home actions. This extraction tool should be fexible and allow for integration of next action. It will for instance feature video and text processing to extract [Subject+ Action + Object] type of data. Investigate methods for HAR Implement a tree model combaning HAR with YOLO to identify agent and objects Evaluate the HAR pipeline with the Toyota Robot HSR  Topics Human Action Recognition,\nPrerequisites  Skills: Python, C++, Git.  References  https://www-sciencedirect-com.wwwproxy1.library.unsw.edu.au/science/article/pii/S174228761930283X https://openaccess.thecvf.com/content_cvpr_2015/papers/Rohrbach_A_Dataset_for_2015_CVPR_paper.pdf https://dl-acm-org.wwwproxy1.library.unsw.edu.au/doi/abs/10.1145/3355390?casa_token=MrZSE8hoPFYAAAAA:rcwHYdISRyLM5OApuN_2SASbwgBsswxx2EPHy9mGP8NaqIdvBj0q5LIa9_ChdyI_Lzfi4GX0PWjhD54 https://prior.allenai.org/projects/charades https://arxiv.org/pdf/1708.02696.pdf https://arxiv.org/pdf/1806.11230.pdf  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"190eaed982907f37b92ff19ced9aea03","permalink":"https://CHRI-Lab.github.io/prospective/undergrad/human_action_recognition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/human_action_recognition/","section":"prospective","summary":"Context Action Recognition is curcial for robots to perfoma around humans. Indeed, robot need to asses human action and intentions in order to assist them in everyday life tasks and to collaborative efficiently.\nThe field of action recognition has aimed to use typical sensors found on robots to recognize agnts, objects and actions they are performing. Typical approach is to record a dataset of various action and label them. But often theses actions are not natural and it can be difficult to represent the variety of ways to perform actions with a lab built dataset. In this project we propose to use audio desription movies to label actions. AD movies integrate a form of narration to allow virually impaired veiwers to undertsnad the visual element sowed on screen. This information often deals with action actually depicted on the scene.\nGoals \u0026amp; Milestones During this project, the student will:\n Develop a pipeline to collect and crop clip of AD movies for at home actions. This extraction tool should be fexible and allow for integration of next action. It will for instance feature video and text processing to extract [Subject+ Action + Object] type of data. Investigate methods for HAR Implement a tree model combaning HAR with YOLO to identify agent and objects Evaluate the HAR pipeline with the Toyota Robot HSR  Topics Human Action Recognition,\nPrerequisites  Skills: Python, C++, Git.  References  https://www-sciencedirect-com.wwwproxy1.library.unsw.edu.au/science/article/pii/S174228761930283X https://openaccess.thecvf.com/content_cvpr_2015/papers/Rohrbach_A_Dataset_for_2015_CVPR_paper.pdf https://dl-acm-org.wwwproxy1.library.unsw.edu.au/doi/abs/10.1145/3355390?casa_token=MrZSE8hoPFYAAAAA:rcwHYdISRyLM5OApuN_2SASbwgBsswxx2EPHy9mGP8NaqIdvBj0q5LIa9_ChdyI_Lzfi4GX0PWjhD54 https://prior.allenai.org/projects/charades https://arxiv.org/pdf/1708.02696.pdf https://arxiv.org/pdf/1806.11230.pdf  ","tags":["machine learning","robotics","cse-honours"],"title":"Human Action Recognition from AD Movies","type":"prospective"},{"authors":null,"categories":null,"content":"Context The field of social human-robot interaction is growing. Understanding how communication between humans (human-human) generalises during human-robot communication is crucial in building fluent and enjoyable interactions with social robots. Everyday, more datasets that feature social interaction between humans and between humans and robots are made freely available online.\nIn this project we propose to take a data-driven approach to build predictive models of social interactions between humans (HH) ( and between humans and robots (HR) interaction using 3 different datasets. Relevant research questions include:\n Which multi-modal features can be transferable from HH to HR setups? Are there common features that discriminate human behaviour in HH or HR scenarios (e.g. \u0026lsquo;Do people speak less or slower with robots?\u0026rsquo; \u0026hellip; )  Goals \u0026amp; Milestones During this project, the student will:\n Explore datasets (PinSoRo, MHHRI and P2PSTORY): type of data (video; audio, point cloud), available labels and annotation \u0026hellip; Extract relevant features multimodal on each dataset Evaluate predictive models for each dataset (i.e. engagement) Explore transfer learning from one dataset to another  There is also potential to use UNSW’s National Facility for Human-Robot Interaction Research to create a new dataset.\nTopics Machine Learning, Human-Robot Interaction\nPrerequisites  Skills: Python, ROS, Git.  References  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205999 https://www.cl.cam.ac.uk/research/rainbow/projects/mhhri/ https://www.media.mit.edu/projects/p2pstory/overview/  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e82d019d099947b47f1a8349d2ec6a2c","permalink":"https://CHRI-Lab.github.io/prospective/undergrad/hri-dataset-mine/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/hri-dataset-mine/","section":"prospective","summary":"Context The field of social human-robot interaction is growing. Understanding how communication between humans (human-human) generalises during human-robot communication is crucial in building fluent and enjoyable interactions with social robots. Everyday, more datasets that feature social interaction between humans and between humans and robots are made freely available online.\nIn this project we propose to take a data-driven approach to build predictive models of social interactions between humans (HH) ( and between humans and robots (HR) interaction using 3 different datasets. Relevant research questions include:\n Which multi-modal features can be transferable from HH to HR setups? Are there common features that discriminate human behaviour in HH or HR scenarios (e.g. \u0026lsquo;Do people speak less or slower with robots?\u0026rsquo; \u0026hellip; )  Goals \u0026amp; Milestones During this project, the student will:\n Explore datasets (PinSoRo, MHHRI and P2PSTORY): type of data (video; audio, point cloud), available labels and annotation \u0026hellip; Extract relevant features multimodal on each dataset Evaluate predictive models for each dataset (i.e. engagement) Explore transfer learning from one dataset to another  There is also potential to use UNSW’s National Facility for Human-Robot Interaction Research to create a new dataset.\nTopics Machine Learning, Human-Robot Interaction\nPrerequisites  Skills: Python, ROS, Git.  References  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205999 https://www.cl.cam.ac.uk/research/rainbow/projects/mhhri/ https://www.media.mit.edu/projects/p2pstory/overview/  ","tags":["machine-learning","hri","cse-honours","affective-computing"],"title":"Machine Learning for Social Interaction Modelling","type":"prospective"},{"authors":null,"categories":null,"content":"Context Social robots are foreseen to be encountered in our everyday life, playing roles as assistant or companion (to mention a few). Recent studies have shown the potential harmful impacts of overtrust in social robotics [1], as robots may collect sensitive information without the user’s knowledge.\nBehavioural styles allow robots to express themselves differently within the same context. Given a specific gesture, keyframe manipulation can be used in order to generate style-based variation to the gesture. Behavioural styles have been studied in the past to improve robot\u0026rsquo;s behaviour during human-robot interaction [2].\nIn this project, we will explore how behavioural styles can influence engagement, trust and persuasion during human-robot interaction.\nGoals \u0026amp; Milestones  Implement behavioural styles for the Nao robot (voice and behaviour) and for a voice assistant (voice only) Design at least two behaviour styles based on human behaviour and personality styles Evaluate and compare these styles via experimentation Design a scenario similar to the one described in paper [3] Setup a data collection environment (posture, video and audio) in the HRI Lab facility of UNSW Select appropriate tasks and/or questionnaires to measure engagement, trust and/or persuasion Evaluate the system via an experiment with users Complete the data analysis  Topics Robotics, HRI, Psychology\nPrerequisites  Skills: Python, ROS and Git.  References  [1]https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2019/10/14081257/Robots_social_impact_eng.pdf [2] Johal, W., Pesty, S., \u0026amp; Calvary, G. (2014, August). Towards companion robots behaving with style. In The 23rd IEEE International Symposium on Robot and Human Interactive Communication (pp. 1063-1068). IEEE. [3] Bainbridge, W. A., Hart, J. W., Kim, E. S., \u0026amp; Scassellati, B. (2011). The benefits of interactions with physically present robots over video-displayed agents. International Journal of Social Robotics, 3(1), 41-52. [4] Peters, R., Broekens, J., Li, K., \u0026amp; Neerincx, M. A. (2019, July). Robot Dominance Expression Through Parameter-based Behaviour Modulation. In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents (pp. 224-226). ACM. [5] Shane Saunderson et al. It Would Make Me Happy if You Used My Guess: Comparing Robot Persuasive Strategies in Social Human–Robot Interaction, IEEE Robotics and Automation Letters (2019). DOI: 10.1109/LRA.2019.2897143  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"19e44b1e4a8ff02a1ff888bd0cd84523","permalink":"https://CHRI-Lab.github.io/prospective/undergrad/behavioural-styles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/behavioural-styles/","section":"prospective","summary":"Context Social robots are foreseen to be encountered in our everyday life, playing roles as assistant or companion (to mention a few). Recent studies have shown the potential harmful impacts of overtrust in social robotics [1], as robots may collect sensitive information without the user’s knowledge.\nBehavioural styles allow robots to express themselves differently within the same context. Given a specific gesture, keyframe manipulation can be used in order to generate style-based variation to the gesture. Behavioural styles have been studied in the past to improve robot\u0026rsquo;s behaviour during human-robot interaction [2].\nIn this project, we will explore how behavioural styles can influence engagement, trust and persuasion during human-robot interaction.\nGoals \u0026amp; Milestones  Implement behavioural styles for the Nao robot (voice and behaviour) and for a voice assistant (voice only) Design at least two behaviour styles based on human behaviour and personality styles Evaluate and compare these styles via experimentation Design a scenario similar to the one described in paper [3] Setup a data collection environment (posture, video and audio) in the HRI Lab facility of UNSW Select appropriate tasks and/or questionnaires to measure engagement, trust and/or persuasion Evaluate the system via an experiment with users Complete the data analysis  Topics Robotics, HRI, Psychology\nPrerequisites  Skills: Python, ROS and Git.  References  [1]https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2019/10/14081257/Robots_social_impact_eng.pdf [2] Johal, W., Pesty, S., \u0026amp; Calvary, G. (2014, August). Towards companion robots behaving with style. In The 23rd IEEE International Symposium on Robot and Human Interactive Communication (pp. 1063-1068). IEEE. [3] Bainbridge, W. A., Hart, J. W., Kim, E. S., \u0026amp; Scassellati, B. (2011). The benefits of interactions with physically present robots over video-displayed agents. International Journal of Social Robotics, 3(1), 41-52. [4] Peters, R., Broekens, J., Li, K., \u0026amp; Neerincx, M. A. (2019, July). Robot Dominance Expression Through Parameter-based Behaviour Modulation. In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents (pp.","tags":["robotics","human-robot interaction","experiment","cse-honours"],"title":"Persuasive Robots - Exploring Behavioural Styles","type":"prospective"},{"authors":null,"categories":null,"content":" Human-Robot Interaction investigates the use of natural communication channels (speech, gestures \u0026hellip;) between robots and humans and finds applications in the industry as well as in personal services such as therapy and education. The state-of the art research in Human-Robot Interaction has mainly explored short term interactions (limited time or limited sessions). Interested in long term impact, our research aims to provide new elements that enable to measure unplugged effect of HRI (e.g. impact outside the interaction). Grounding our research on human-human interaction and human-computer interaction (related to healthcare, education or cognitive sciences), our work will feature AI and data-driven approaches and will lead to empirical evaluations of the proposed systems.\nTechnical and theoretical contributions will be valued and potentially provide impact in both research and society. Contributions can be focused in modelling the interaction, enabling autonomous and adapted interaction or designing new interaction types.\nProfile of the candidate You must have Bachelor or Masters degree in Computer Sciences or a related field (Mathematics, Electrical or Mechanical). Good programming skills are required (C++, Python and others). Training and skills in AI, machine learning or robotics is preferable. Knowledge of classical programming frameworks in these domains is appreciated (PyTorch, Tf, ROS). English proficiency is required. The PhD position is highly interdisciplinary and requires an understanding and/or interest in social sciences.\nFurther details Depending on the research focus, joint supervisors specialised in Design, Robotics, AI, HCI or Education, will be invited. UNSW and CSE will provide you to access to the HCI and Robotics Labs, GPU clusters and the HRI experimental facility equipped with more than 200 sensors (available to run studies).\nUNSW Sydney is in the top 100 universities in the world, with more than 59,000 students and a strong research community. Located in Sydney, Australia’s largest city, the University was established in 1949 with a unique focus on scientific, technological and professional disciplines. At UNSW, you will benefit from an international and vibrant research community in a pleasant environment.\nThe net amount of the scholarship will be approximately AUD 2300 per month, increasing annually. You will also receive a holiday allowance. Additional financial support is available for attending conferences and workshops. There will also be possibilities for a research visit (industry or international) during the PhD.\nQueries For informal queries, do not hesitate to contact wafa /dot/ johal /at/ unsw.edu.au. Your application should include:\n A letter highlighting your motivation for your application including: Why do you wish to pursue a PhD in human-robot interaction? What prior experience do you have? How would you approach human-robot interaction research? Please also indicate your research interests and whether you have any external funding for covering the UNSW tuition fee (see section below) and living costs. A CV, with copies of relevant grades, masters thesis work or publications. The names and contact details of at least 2 referees.  Selected candidates will be invited for interview, which can be organised over Skype if necessary.\n Tuition Scholarships To enrol in the PhD program in computer science at UNSW, you are expected to have a degree that is equivalent to a First Class Honours degree in Computer Science, Mathematics, or a related field. Obtaining a PhD scholarship from UNSW is a very competitive process. UNSW\u0026rsquo;s online self-assessment tool gives a good indication of how competitive you are for scholarships. PhD students under my supervision are often eligible for top-up scholarships from Data61.\nAlong with the documents mentioned above, please send me the results of the online self-assessment tool.\nAll scholarship applicants need to prepare a research proposal in consultation with the prospective supervisor (around 500 words). The selection also usually takes into account your grades (mainly for the last 2 years of study), the ranking of your previous university, and your publications (if any).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6b07f4aefb9ac0a9f23dbaec9a52fd74","permalink":"https://CHRI-Lab.github.io/prospective/phd/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/phd/","section":"prospective","summary":"UNSW PhD program \u0026 Scholarships","tags":["position","social HRI","applied machine learning","user experiments"],"title":"PhD Position Available!","type":"prospective"},{"authors":null,"categories":null,"content":"Context Handwriting is one of the most important motor skill we learn as human. Children with handwriting difficulties can find their academic impacted and often succeed less at school.\nIn this project we propose to explore new methods for a robot to write using a pen. The project will use the Fetch Robot and aim to evaluate how a robot could learn handwriting trajectories using human demonstrations.\nGoals \u0026amp; Milestones During this project, the student will:\n Learn about ROS Develop a ROS package that enables to control the Fetch Robot arm given a series of handwriting strokes Explore the use of different methods for the robot to learn letter writing from demonstrations Evaluate the implemented method compare to other state of the art methods  Topics ROS, Learning by Demonstration, Robotics, Handwriting\nPrerequisites  Skills: Python, C++, ROS, Git.  References See Zotero Collection https://www.zotero.org/groups/2419050/hri-unsw/collections/GQERYTFZ\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"814e3f6b844a10aed2e5bd78ea0b4b88","permalink":"https://CHRI-Lab.github.io/prospective/undergrad/writing_2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/writing_2020/","section":"prospective","summary":"Context Handwriting is one of the most important motor skill we learn as human. Children with handwriting difficulties can find their academic impacted and often succeed less at school.\nIn this project we propose to explore new methods for a robot to write using a pen. The project will use the Fetch Robot and aim to evaluate how a robot could learn handwriting trajectories using human demonstrations.\nGoals \u0026amp; Milestones During this project, the student will:\n Learn about ROS Develop a ROS package that enables to control the Fetch Robot arm given a series of handwriting strokes Explore the use of different methods for the robot to learn letter writing from demonstrations Evaluate the implemented method compare to other state of the art methods  Topics ROS, Learning by Demonstration, Robotics, Handwriting\nPrerequisites  Skills: Python, C++, ROS, Git.  References See Zotero Collection https://www.zotero.org/groups/2419050/hri-unsw/collections/GQERYTFZ","tags":["ROS","robotics","handwriting","co-writer","cse-honours","active"],"title":"Robot Writing","type":"prospective"},{"authors":null,"categories":null,"content":"Join the group Project overview Enabling robots to navigate in indoors environments in a safe and socially acceptable manner around groups of humans is still an open research area. Socially aware navigation considers the multi-modal assessment of the group dynamics, group formation inferring, path planning, real-time path adaptation, and human-robot communication. Up to now the research in the field has considered a couple of classical scenarios such as crossing a group in a corridor or passing a door. Limitations in terms of lack of realistic datasets is often mentioned in the field. In this research project, we will aim to design several scenarios involving groups of humans in realistic settings. Our work will focus on three main tasks for the robot:\n approach and join a group, 2) passing by a group and 3) greeting a group. A first step of the project will be to record a novel dataset with rich interactions between the humans (H-H scenarios) and between humans and a teleoperated robot (H-R scenarios). The dataset will be collected at the HRI facility allowing multimodal synchronous recording. After that, a new model for path planning} will be developed. For the model, we will explore rule-based constraints (i.e. not passing between two persons speaking together) and learned constrained using the dataset recorded to infer implicit social norms. Finally, the model will be tested empirically with new users in which the robot will have to take real-time path planning decisions.  Schedule   Expected Outcomes  A dataset featuring different scenarios of groups and spatial interactions will be recorded at the HRI facility in Paddington. After anonymization, the dataset will be made open. Development of a novel socially aware module allowing the robot to approach and leave groups using a hybrid method (rule-based and data-driven)  Empirical evaluation with end-users in the National Facility for HRI   Report or conference publication at CoRL (Conference on Robot Learning – July 2021) or at the Robotics and Automation-Letters (RA-L) The code implemented during this project must be fully documented  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f1129a74b5048ed65ce983632037ebe8","permalink":"https://CHRI-Lab.github.io/project/social_navigation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/social_navigation/","section":"project","summary":"Investigate how robots could be made aware of social navigation","tags":["social navigation","human-robot interaction","group"],"title":"Rosona - Robot's Social Navigation","type":"project"},{"authors":null,"categories":null,"content":"Context While digital tools are more and more used in classrooms, teachers' common practice remains to use photocopied paper documents to share and collect learning exercises from their students. With the Tangible e-Ink Paper (TIP) system, we aim to explore the use of tangible manipulatives interacting with paper sheets as a bridge between digital and paper traces of learning. Featuring an e-Ink display, a paper-based localisation system and a wireless connection, TIPs are envisioned to be used as a versatile tool across various curriculum activities.\nGoals \u0026amp; Milestones  Literature Review on Tangible UI (TUI) in Education Implement and test a proof of concept of TIPs for learning Assemble 3 TIPs (3D printing of parts, soldering, etc.) Install libraries on Rasberry PI ( e.g. libdots - used for self-paper-based localisation, bluetooth communication) Develop two demo applications using TIPs for  individual work (on A4 sheet of paper) collaborative work (on min A2)    Topics Tangible User Interfaces, HCI\nPrerequisites and Learning Outcomes  Skills: Javascript, Python or C++. Git. Qt, Rasberry Pi  References  https://infoscience.epfl.ch/record/271833/files/paper.pdf https://infoscience.epfl.ch/record/224129/files/paper.pdf https://infoscience.epfl.ch/record/270077/files/Robot_Analytics.pdf https://link.springer.com/chapter/10.1007/978-3-030-29736-7_57  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4319114a2a5ebb9ad7aa543b433c590c","permalink":"https://CHRI-Lab.github.io/prospective/undergrad/tip_2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/tip_2020/","section":"prospective","summary":"Context While digital tools are more and more used in classrooms, teachers' common practice remains to use photocopied paper documents to share and collect learning exercises from their students. With the Tangible e-Ink Paper (TIP) system, we aim to explore the use of tangible manipulatives interacting with paper sheets as a bridge between digital and paper traces of learning. Featuring an e-Ink display, a paper-based localisation system and a wireless connection, TIPs are envisioned to be used as a versatile tool across various curriculum activities.\nGoals \u0026amp; Milestones  Literature Review on Tangible UI (TUI) in Education Implement and test a proof of concept of TIPs for learning Assemble 3 TIPs (3D printing of parts, soldering, etc.) Install libraries on Rasberry PI ( e.g. libdots - used for self-paper-based localisation, bluetooth communication) Develop two demo applications using TIPs for  individual work (on A4 sheet of paper) collaborative work (on min A2)    Topics Tangible User Interfaces, HCI\nPrerequisites and Learning Outcomes  Skills: Javascript, Python or C++. Git. Qt, Rasberry Pi  References  https://infoscience.epfl.ch/record/271833/files/paper.pdf https://infoscience.epfl.ch/record/224129/files/paper.pdf https://infoscience.epfl.ch/record/270077/files/Robot_Analytics.pdf https://link.springer.com/chapter/10.1007/978-3-030-29736-7_57  ","tags":["tangible-interfaces","education","cse-honours"],"title":"Tangible e-Ink Paper Interfaces for Learners","type":"prospective"},{"authors":null,"categories":null,"content":"Context: Visuo-Motor coordination problems can impair children in their academic achievements and in their everyday life. Gross visuo-motor skills, in particular, are required in a range of social and educational activities that contribute to children\u0026rsquo;s physical and cognitive development such as playing a musical instrument, ball-based sports or dancing. Children with visuo-motor coordination difficulties are typically diagnosed with developmental coordination disorder or cerebral palsy and need undergo physical therapy. The therapy sessions are often not engaging for children and conducted individually. In this project, we aim to design new forms of interaction with a swarm for enhance visuo-motor coordination. We propose to develop a game that allows multiple children to play collaboratively on the same table.\nGoals \u0026amp; Milestones  Implement the set of basic swarm behaviour using 4 Cellulo robots Integrate collaorative and tangible interactions Test the system with a participants. We plan ti integrate a measure of cognitive load using eye tracking data  Topics HCI, Health, Game, Swarm Robotics\nPrerequisites  Skills: Python, C++, Js  References  https://www.epfl.ch/labs/chili/index-html/research/cellulo/  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ff4b56ea1caf8a7cc2de574775ac7d8e","permalink":"https://CHRI-Lab.github.io/prospective/undergrad/h-swarm_2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/h-swarm_2020/","section":"prospective","summary":"Context: Visuo-Motor coordination problems can impair children in their academic achievements and in their everyday life. Gross visuo-motor skills, in particular, are required in a range of social and educational activities that contribute to children\u0026rsquo;s physical and cognitive development such as playing a musical instrument, ball-based sports or dancing. Children with visuo-motor coordination difficulties are typically diagnosed with developmental coordination disorder or cerebral palsy and need undergo physical therapy. The therapy sessions are often not engaging for children and conducted individually. In this project, we aim to design new forms of interaction with a swarm for enhance visuo-motor coordination. We propose to develop a game that allows multiple children to play collaboratively on the same table.\nGoals \u0026amp; Milestones  Implement the set of basic swarm behaviour using 4 Cellulo robots Integrate collaorative and tangible interactions Test the system with a participants. We plan ti integrate a measure of cognitive load using eye tracking data  Topics HCI, Health, Game, Swarm Robotics\nPrerequisites  Skills: Python, C++, Js  References  https://www.epfl.ch/labs/chili/index-html/research/cellulo/  ","tags":["cellulo","tangible","swarm","cse-honours"],"title":"Tangible Human Swarm Interaction","type":"prospective"},{"authors":null,"categories":null,"content":"Context: Online learning presents several advantages: decreasing cost, allowing more flexibility and access to far away training resources. However, studies have found that it also limits communications between peers and teachers, limits physical interactions and that it requires a big commitment on the student\u0026rsquo;s part to plan and stay assiduous in their learning.\nGoals \u0026amp; Milestones In this project, we aim to design and test a novel way to engage students in collaborative online learning by using haptic enabled tangible robots. The project will consist in:\n developing a tool allowing the design of online activities for two or more robots to be connected implementing a demonstrator for this new library that will embed a series of small exercises hilightling the new capability of remote haptic-assisted collaboration evaluating the demonstrator with a user experiment  Topics HCI, Haptics, Robot, Collaborative Work (Training/Gaminig)\nPrerequisites  Skills: C++, Js,  References See Zotero Collection https://www.zotero.org/groups/2419050/hri-unsw/collections/JXBHFMBC\n Schneider, B., Jermann, P., Zufferey, G., \u0026amp; Dillenbourg, P. (2011). Benefits of a Tangible Interface for Collaborative Learning and Interaction. IEEE Transactions on Learning Technologies, 4(3), 222–232. https://doi.org/10.1109/TLT.2010.36 Asselborn, T., Guneysu, A., Mrini, K., Yadollahi, E., Ozgur, A., Johal, W., \u0026amp; Dillenbourg, P. (2018). Bringing letters to life: Handwriting with haptic-enabled tangible robots. Proceedings of the 17th ACM Conference on Interaction Design and Children, 219–230. East, B., DeLong, S., Manshaei, R., Arif, A., \u0026amp; Mazalek, A. (2016). Actibles: Open Source Active Tangibles. Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces, 469–472. https://doi.org/10.1145/2992154.2996874 Guinness, D., Muehlbradt, A., Szafir, D., \u0026amp; Kane, S. K. (2019a). RoboGraphics: Dynamic Tactile Graphics Powered by Mobile Robots. The 21st International ACM SIGACCESS Conference on Computers and Accessibility, 318–328. https://doi.org/10.1145/3308561.3353804 Guinness, D., Muehlbradt, A., Szafir, D., \u0026amp; Kane, S. K. (2019b). RoboGraphics: Using Mobile Robots to Create Dynamic Tactile Graphics. The 21st International ACM SIGACCESS Conference on Computers and Accessibility, 673–675. https://doi.org/10.1145/3308561.3354597 Guinness, D., Muehlbradt, A., Szafir, D., \u0026amp; Kane, S. K. (2018). The Haptic Video Player: Using Mobile Robots to Create Tangible Video Annotations. Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces, 203–211. https://doi.org/10.1145/3279778.3279805 Guneysu, A., Johal, W., Ozgur, A., \u0026amp; Dillenbourg, P. (2018). Tangible Robots Mediated Collaborative Rehabilitation Design: Can we Find Inspiration from Scripting Collaborative Learning? Workshop on Robots for Learning R4L HRI2018. Guneysu Ozgur, A., Wessel, M. J., Johal, W., Sharma, K., Ozgur, A., Vuadens, P., Mondada, F., Hummel, F. C., \u0026amp; Dillenbourg, P. (2018). Iterative design of an upper limb rehabilitation game with tangible robots. Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, 241–250. Guneysu Ozgur, A., Wessel, M. J., Olsen, J. K., Johal, W., Özgür, A., Hummel, F. C., \u0026amp; Dillenbourg, P. (2020). Gamified Motor Training with Tangible Robots in Older Adults: A Feasibility Study and Comparison with Young. Frontiers in Aging Neuroscience, 12. https://doi.org/10.3389/fnagi.2020.00059 Ishii, H., \u0026amp; Ullmer, B. (1997). Tangible Bits: Towards Seamless Interfaces Between People, Bits and Atoms. Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 234–241. https://doi.org/10.1145/258549.258715 Johal, W., Tran, A., Khodr, H., Özgür, A., \u0026amp; Dillenbourg, P. (2019). TIP: Tangible e-Ink Paper Manipulatives for Classroom Orchestration. Proceedings of the 31st Australian Conference on Human-Computer-Interaction, 595–598. https://doi.org/10.1145/3369457.3369539 Loparev, A., Westendorf, L., Flemings, M., Cho, J., Littrell, R., Scholze, A., \u0026amp; Shaer, O. (2017). BacPack: Exploring the Role of Tangibles in a Museum Exhibit for Bio-Design. Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction, 111–120. https://doi.org/10.1145/3024969.3025000 Okerlund, J., Shaer, O., \u0026amp; Latulipe, C. (2016). Teaching Computational Thinking Through Bio-Design (Abstract Only). Proceedings of the 47th ACM Technical Symposium on Computing Science Education, 698. https://doi.org/10.1145/2839509.2850569 O’Malley, C., \u0026amp; Fraser, D. S. (2004). Literature review in learning with tangible technologies. Ozgur, A. G., Wessel, M. J., Asselborn, T., Olsen, J. K., Johal, W., Özgür, A., Hummel, F. C., \u0026amp; Dillenbourg, P. (2019). Designing Configurable Arm Rehabilitation Games: How Do Different Game Elements Affect User Motion Trajectories? 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 5326–5330. https://doi.org/10.1109/EMBC.2019.8857508 Ozgur, A., Johal, W., Mondada, F., \u0026amp; Dillenbourg, P. (2017). Haptic-enabled handheld mobile robots: Design and analysis. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 2449–2461. Ozgur, A., Lemaignan, S., Johal, W., Beltran, M., Briod, M., Pereyre, L., Mondada, F., \u0026amp; Dillenbourg, P. (2017). Cellulo: Versatile handheld robots for education. 2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI, 119–127.  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d1d93155497c3217eaa38e6d820f3236","permalink":"https://CHRI-Lab.github.io/prospective/undergrad/tangible_online/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/tangible_online/","section":"prospective","summary":"Context: Online learning presents several advantages: decreasing cost, allowing more flexibility and access to far away training resources. However, studies have found that it also limits communications between peers and teachers, limits physical interactions and that it requires a big commitment on the student\u0026rsquo;s part to plan and stay assiduous in their learning.\nGoals \u0026amp; Milestones In this project, we aim to design and test a novel way to engage students in collaborative online learning by using haptic enabled tangible robots. The project will consist in:\n developing a tool allowing the design of online activities for two or more robots to be connected implementing a demonstrator for this new library that will embed a series of small exercises hilightling the new capability of remote haptic-assisted collaboration evaluating the demonstrator with a user experiment  Topics HCI, Haptics, Robot, Collaborative Work (Training/Gaminig)\nPrerequisites  Skills: C++, Js,  References See Zotero Collection https://www.zotero.org/groups/2419050/hri-unsw/collections/JXBHFMBC\n Schneider, B., Jermann, P., Zufferey, G., \u0026amp; Dillenbourg, P. (2011). Benefits of a Tangible Interface for Collaborative Learning and Interaction. IEEE Transactions on Learning Technologies, 4(3), 222–232. https://doi.org/10.1109/TLT.2010.36 Asselborn, T., Guneysu, A., Mrini, K., Yadollahi, E., Ozgur, A., Johal, W., \u0026amp; Dillenbourg, P. (2018). Bringing letters to life: Handwriting with haptic-enabled tangible robots. Proceedings of the 17th ACM Conference on Interaction Design and Children, 219–230. East, B., DeLong, S., Manshaei, R., Arif, A., \u0026amp; Mazalek, A. (2016). Actibles: Open Source Active Tangibles. Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces, 469–472. https://doi.org/10.1145/2992154.2996874 Guinness, D., Muehlbradt, A., Szafir, D., \u0026amp; Kane, S. K. (2019a). RoboGraphics: Dynamic Tactile Graphics Powered by Mobile Robots. The 21st International ACM SIGACCESS Conference on Computers and Accessibility, 318–328. https://doi.org/10.1145/3308561.3353804 Guinness, D., Muehlbradt, A., Szafir, D., \u0026amp; Kane, S. K. (2019b). RoboGraphics: Using Mobile Robots to Create Dynamic Tactile Graphics.","tags":["cellulo","tangible","collaborative learning","cse-honours"],"title":"Tangible Robots for Collaborative Online Learning","type":"prospective"},{"authors":null,"categories":null,"content":"Context Natural language is an important part of communication since it offers an intuitive and efficient way of conveying ideas to another individual. Enabling robots to efficiently use language is essential for human-robot collaboration. In this project, we aim to develop an interface between a dialog manager (i.e. DialogFlow) and ROS (Robotics Operating System). By doing this, we will be able to use the powerful dialogue systems in human-robot interaction scenario.\nA scenario, using tangible robots (Cellulo) combined with voice assistant for upper-arm rehabilitation will be implemented to show the potential of this new ros-package.\nGoals \u0026amp; Milestones During this project, the student will:\n Learn about Google DialogFlow and ROS Develop a ROS package that enables to access and manipulates DialogFlow features Develop a Cellulo Rehabilitation Game Test the game with a pilot experiment  Topics Voice-Assistant, Human-Robot Interaction, ROS\nPrerequisites  Skills: Python, C++, ROS, Git.  References  https://dialogflow.com/ https://www.ros.org/ http://wafa.johal.org/project/cellulo/ Hudson, C., Bethel, C. L., Carruth, D. W., Pleva, M., Juhar, J., \u0026amp; Ondas, S. (2017, October). A training tool for speech driven human-robot interaction applications. In 2017 15th International Conference on Emerging eLearning Technologies and Applications (ICETA) (pp. 1-6). IEEE. Moore, R. K. (2017). Is spoken language all-or-nothing? Implications for future speech-based human-machine interaction. In Dialogues with Social Robots (pp. 281-291). Springer, Singapore. Beirl, D., Yuill, N., \u0026amp; Rogers, Y. (2019). Using Voice Assistant Skills in Family Life. In Lund, K., Niccolai, G. P., Lavoué, E., Gweon, C. H., \u0026amp; Baker, M. (Eds.), A Wide Lens: Combining Embodied, Enactive, Extended, and Embedded Learning in Collaborative Settings, 13th International Conference on Computer Supported Collaborative Learning (CSCL) 2019, Volume 1 (pp. 96-103). Lyon, France: International Society of the Learning Sciences.  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"52562cd547497343def7c8bf9ba1c493","permalink":"https://CHRI-Lab.github.io/prospective/undergrad/voice-robot/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/prospective/undergrad/voice-robot/","section":"prospective","summary":"Context Natural language is an important part of communication since it offers an intuitive and efficient way of conveying ideas to another individual. Enabling robots to efficiently use language is essential for human-robot collaboration. In this project, we aim to develop an interface between a dialog manager (i.e. DialogFlow) and ROS (Robotics Operating System). By doing this, we will be able to use the powerful dialogue systems in human-robot interaction scenario.\nA scenario, using tangible robots (Cellulo) combined with voice assistant for upper-arm rehabilitation will be implemented to show the potential of this new ros-package.\nGoals \u0026amp; Milestones During this project, the student will:\n Learn about Google DialogFlow and ROS Develop a ROS package that enables to access and manipulates DialogFlow features Develop a Cellulo Rehabilitation Game Test the game with a pilot experiment  Topics Voice-Assistant, Human-Robot Interaction, ROS\nPrerequisites  Skills: Python, C++, ROS, Git.  References  https://dialogflow.com/ https://www.ros.org/ http://wafa.johal.org/project/cellulo/ Hudson, C., Bethel, C. L., Carruth, D. W., Pleva, M., Juhar, J., \u0026amp; Ondas, S. (2017, October). A training tool for speech driven human-robot interaction applications. In 2017 15th International Conference on Emerging eLearning Technologies and Applications (ICETA) (pp. 1-6). IEEE. Moore, R. K. (2017). Is spoken language all-or-nothing? Implications for future speech-based human-machine interaction. In Dialogues with Social Robots (pp. 281-291). Springer, Singapore. Beirl, D., Yuill, N., \u0026amp; Rogers, Y. (2019). Using Voice Assistant Skills in Family Life. In Lund, K., Niccolai, G. P., Lavoué, E., Gweon, C. H., \u0026amp; Baker, M. (Eds.), A Wide Lens: Combining Embodied, Enactive, Extended, and Embedded Learning in Collaborative Settings, 13th International Conference on Computer Supported Collaborative Learning (CSCL) 2019, Volume 1 (pp. 96-103). Lyon, France: International Society of the Learning Sciences.  ","tags":["ROS","robotics","voice-assistant","cellulo","cse-honours"],"title":"Voice for ROS","type":"prospective"}]