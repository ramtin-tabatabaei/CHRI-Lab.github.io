<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>cellulo | computational human-robot interaction lab</title>
    <link>https://CHRI-Lab.github.io/tag/cellulo/</link>
      <atom:link href="https://CHRI-Lab.github.io/tag/cellulo/index.xml" rel="self" type="application/rss+xml" />
    <description>cellulo</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Research Group in HRI at CSE UNSW</copyright><lastBuildDate>Mon, 01 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://CHRI-Lab.github.io/images/icon_hu02c0217b7eeb0e7fdb8b09af6bff09b6_8240_512x512_fill_lanczos_center_2.png</url>
      <title>cellulo</title>
      <link>https://CHRI-Lab.github.io/tag/cellulo/</link>
    </image>
    
    <item>
      <title>Using Tabletop Robots to promote Inclusive Classroom Experiences</title>
      <link>https://CHRI-Lab.github.io/publication/neto-using-2020/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/publication/neto-using-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cellulo</title>
      <link>https://CHRI-Lab.github.io/project/cellulo/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/project/cellulo/</guid>
      <description>&lt;p&gt;With the Cellulo project [6], a part of the Transversal Educational Activities of the NCCR Robotics, we introduced a
new robotic platform which is small and easily deployable. A dotted pattern printed on regular paper enables the
Cellulo robots with absolute localization with a precision of 270 microns [7]. The robots also have a new
locomotion system with a drive relying on a permanent magnet to actuate coated metal balls [8]. This new drive
design allows backdrivability; i.e. it allows the robot to move and to be moved without damaging it. With this
system, we also implemented a haptic feedback modality, allowing the user to feel forces when grasping the robot
[9].
The robots are connected via Bluetooth to a master (PC or tablet) that handles the logical and computation of the
activity. The onboard PCB of the robots only allows for proceeding the localization (image capture and decoding of
the pattern) and the control of the three wheels actuation.
During two years, we developed several learning activities using the robots. The Figure for example shows the Feel
the Wind activity, in which the learners were taught that the wind was formed by air moving from with high to low
pressure points.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/zv6nDMQCWCo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;In the Cellulo project, we also started to explore the use of haptic feedback for learners. Haptic feedback enables us to
render forces, but also borders, angles, or points. We developed a series of haptic capabilities and small interaction
tasks that can be included in learning activities to inform the learner [9]. We tested the haptic feedback with
children for instance in the symmetry activity, in which the child is able to formulate hypothesis on the placement
of the symmetrical shape and to verify their claims by feel haptically the shape on paper (left Figure). We also tested
with some pilot with visually impaired children who were able to explore a map of their classroom using the Cellulo
robots.
Research Perspectives for Tangible Swarm Interaction and Haptic for Learners: We are now exploring the
dynamics of the group of learners in manipulating the robots. The collaboration among learner is not always
optimal, and a challenge would be to use the swarm robots to analyses and regulate the collaboration among
learners. As these shared resource can be intelligent agents, they could rearrange themselves according to the
collaborative state of the group.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/g_7glQmTIVo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/s3KAoQNUPZs&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Tangible Human Swarm Interaction</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/h-swarm_2020/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/h-swarm_2020/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://CHRI-Lab.github.io/img/student-projects/swarm.png&#34; alt=&#34;graph&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context:&lt;/h2&gt;
&lt;p&gt;Visuo-Motor coordination problems can impair children in  their academic achievements and in their everyday life. 
Gross visuo-motor skills, in particular, are required in a range of social and educational activities that contribute to children&amp;rsquo;s physical and cognitive development such as playing a musical instrument, ball-based sports or dancing. 
Children with visuo-motor coordination difficulties are typically diagnosed with developmental coordination disorder  or cerebral palsy and need undergo physical therapy. 
The therapy sessions are often not engaging for children and conducted individually. 
In this project, we aim to design new forms of interaction with a swarm for enhance visuo-motor coordination. We propose to develop a game that allows multiple children to play collaboratively on the same table.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implement the set of basic swarm behaviour using 4 Cellulo robots&lt;/li&gt;
&lt;li&gt;Integrate collaorative and tangible interactions&lt;/li&gt;
&lt;li&gt;Test the system with a participants. We plan ti integrate a measure of cognitive load using eye tracking data&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;HCI, Health, Game, Swarm Robotics&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, C++, Js&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.epfl.ch/labs/chili/index-html/research/cellulo/&#34;&gt;https://www.epfl.ch/labs/chili/index-html/research/cellulo/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Tangible Robots for Collaborative Online Learning</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/tangible_online/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/tangible_online/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://CHRI-Lab.github.io/img/cellulo.png&#34; alt=&#34;graph&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context:&lt;/h2&gt;
&lt;p&gt;Online learning  presents several advantages: decreasing cost, allowing more flexibility and access to far away training resources. However, studies have found that it also limits communications between peers and teachers, limits physical interactions and that it requires a big commitment on the student&amp;rsquo;s part to plan and stay assiduous in their learning.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;In this project, we aim to design and test a novel way to engage students in collaborative online learning by using haptic enabled tangible robots.
The project will consist in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;developing a tool allowing the design of online activities for two or more robots to be connected&lt;/li&gt;
&lt;li&gt;implementing a demonstrator for this new library that will embed a series of small exercises hilightling the new capability of remote haptic-assisted collaboration&lt;/li&gt;
&lt;li&gt;evaluating the demonstrator with a user experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;HCI, Haptics, Robot, Collaborative Work (Training/Gaminig)&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: C++, Js,&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;See Zotero Collection &lt;a href=&#34;https://www.zotero.org/groups/2419050/hri-unsw/collections/JXBHFMBC&#34;&gt;https://www.zotero.org/groups/2419050/hri-unsw/collections/JXBHFMBC&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Schneider, B., Jermann, P., Zufferey, G., &amp;amp; Dillenbourg, P. (2011). Benefits of a Tangible Interface for Collaborative Learning and Interaction. IEEE Transactions on Learning Technologies, 4(3), 222–232. &lt;a href=&#34;https://doi.org/10.1109/TLT.2010.36&#34;&gt;https://doi.org/10.1109/TLT.2010.36&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Asselborn, T., Guneysu, A., Mrini, K., Yadollahi, E., Ozgur, A., Johal, W., &amp;amp; Dillenbourg, P. (2018). Bringing letters to life: Handwriting with haptic-enabled tangible robots. Proceedings of the 17th ACM Conference on Interaction Design and Children, 219–230.&lt;/li&gt;
&lt;li&gt;East, B., DeLong, S., Manshaei, R., Arif, A., &amp;amp; Mazalek, A. (2016). Actibles: Open Source Active Tangibles. Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces, 469–472. &lt;a href=&#34;https://doi.org/10.1145/2992154.2996874&#34;&gt;https://doi.org/10.1145/2992154.2996874&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guinness, D., Muehlbradt, A., Szafir, D., &amp;amp; Kane, S. K. (2019a). RoboGraphics: Dynamic Tactile Graphics Powered by Mobile Robots. The 21st International ACM SIGACCESS Conference on Computers and Accessibility, 318–328. &lt;a href=&#34;https://doi.org/10.1145/3308561.3353804&#34;&gt;https://doi.org/10.1145/3308561.3353804&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guinness, D., Muehlbradt, A., Szafir, D., &amp;amp; Kane, S. K. (2019b). RoboGraphics: Using Mobile Robots to Create Dynamic Tactile Graphics. The 21st International ACM SIGACCESS Conference on Computers and Accessibility, 673–675. &lt;a href=&#34;https://doi.org/10.1145/3308561.3354597&#34;&gt;https://doi.org/10.1145/3308561.3354597&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guinness, D., Muehlbradt, A., Szafir, D., &amp;amp; Kane, S. K. (2018). The Haptic Video Player: Using Mobile Robots to Create Tangible Video Annotations. Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces, 203–211. &lt;a href=&#34;https://doi.org/10.1145/3279778.3279805&#34;&gt;https://doi.org/10.1145/3279778.3279805&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guneysu, A., Johal, W., Ozgur, A., &amp;amp; Dillenbourg, P. (2018). Tangible Robots Mediated Collaborative Rehabilitation Design: Can we Find Inspiration from Scripting Collaborative Learning? Workshop on Robots for Learning R4L HRI2018.&lt;/li&gt;
&lt;li&gt;Guneysu Ozgur, A., Wessel, M. J., Johal, W., Sharma, K., Ozgur, A., Vuadens, P., Mondada, F., Hummel, F. C., &amp;amp; Dillenbourg, P. (2018). Iterative design of an upper limb rehabilitation game with tangible robots. Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, 241–250.&lt;/li&gt;
&lt;li&gt;Guneysu Ozgur, A., Wessel, M. J., Olsen, J. K., Johal, W., Özgür, A., Hummel, F. C., &amp;amp; Dillenbourg, P. (2020). Gamified Motor Training with Tangible Robots in Older Adults: A Feasibility Study and Comparison with Young. Frontiers in Aging Neuroscience, 12. &lt;a href=&#34;https://doi.org/10.3389/fnagi.2020.00059&#34;&gt;https://doi.org/10.3389/fnagi.2020.00059&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ishii, H., &amp;amp; Ullmer, B. (1997). Tangible Bits: Towards Seamless Interfaces Between People, Bits and Atoms. Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 234–241. &lt;a href=&#34;https://doi.org/10.1145/258549.258715&#34;&gt;https://doi.org/10.1145/258549.258715&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Johal, W., Tran, A., Khodr, H., Özgür, A., &amp;amp; Dillenbourg, P. (2019). TIP: Tangible e-Ink Paper Manipulatives for Classroom Orchestration. Proceedings of the 31st Australian Conference on Human-Computer-Interaction, 595–598. &lt;a href=&#34;https://doi.org/10.1145/3369457.3369539&#34;&gt;https://doi.org/10.1145/3369457.3369539&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Loparev, A., Westendorf, L., Flemings, M., Cho, J., Littrell, R., Scholze, A., &amp;amp; Shaer, O. (2017). BacPack: Exploring the Role of Tangibles in a Museum Exhibit for Bio-Design. Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction, 111–120. &lt;a href=&#34;https://doi.org/10.1145/3024969.3025000&#34;&gt;https://doi.org/10.1145/3024969.3025000&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Okerlund, J., Shaer, O., &amp;amp; Latulipe, C. (2016). Teaching Computational Thinking Through Bio-Design (Abstract Only). Proceedings of the 47th ACM Technical Symposium on Computing Science Education, 698. &lt;a href=&#34;https://doi.org/10.1145/2839509.2850569&#34;&gt;https://doi.org/10.1145/2839509.2850569&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;O’Malley, C., &amp;amp; Fraser, D. S. (2004). Literature review in learning with tangible technologies.&lt;/li&gt;
&lt;li&gt;Ozgur, A. G., Wessel, M. J., Asselborn, T., Olsen, J. K., Johal, W., Özgür, A., Hummel, F. C., &amp;amp; Dillenbourg, P. (2019). Designing Configurable Arm Rehabilitation Games: How Do Different Game Elements Affect User Motion Trajectories? 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 5326–5330. &lt;a href=&#34;https://doi.org/10.1109/EMBC.2019.8857508&#34;&gt;https://doi.org/10.1109/EMBC.2019.8857508&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ozgur, A., Johal, W., Mondada, F., &amp;amp; Dillenbourg, P. (2017). Haptic-enabled handheld mobile robots: Design and analysis. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 2449–2461.&lt;/li&gt;
&lt;li&gt;Ozgur, A., Lemaignan, S., Johal, W., Beltran, M., Briod, M., Pereyre, L., Mondada, F., &amp;amp; Dillenbourg, P. (2017). Cellulo: Versatile handheld robots for education. 2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI, 119–127.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Voice for ROS</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/voice-robot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/voice-robot/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Natural language is an important part of communication since it offers an intuitive and efficient way of conveying ideas to another individual. Enabling robots to efficiently use language is essential for human-robot collaboration. In this project, we aim to develop an interface between a dialog manager (i.e. DialogFlow) and ROS (Robotics Operating System). By doing this, we will be able to use the powerful dialogue systems in human-robot interaction scenario.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1066/1*OCuPx7AmWofWptQdPN-TPA.png&#34; alt=&#34;robot-va&#34;&gt;&lt;/p&gt;
&lt;p&gt;A scenario, using tangible robots (Cellulo) combined with voice assistant for upper-arm rehabilitation will be implemented to show the potential of this new ros-package.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;During this project, the student will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn about Google DialogFlow and ROS&lt;/li&gt;
&lt;li&gt;Develop a ROS package that enables to access and manipulates DialogFlow features&lt;/li&gt;
&lt;li&gt;Develop a Cellulo Rehabilitation Game&lt;/li&gt;
&lt;li&gt;Test the game with a pilot experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Voice-Assistant, Human-Robot Interaction, ROS&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, C++,  ROS, Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://dialogflow.com/&#34;&gt;https://dialogflow.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ros.org/&#34;&gt;https://www.ros.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wafa.johal.org/project/cellulo/&#34;&gt;http://wafa.johal.org/project/cellulo/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hudson, C., Bethel, C. L., Carruth, D. W., Pleva, M., Juhar, J., &amp;amp; Ondas, S. (2017, October). A training tool for speech driven human-robot interaction applications. In 2017 15th International Conference on Emerging eLearning Technologies and Applications (ICETA) (pp. 1-6). IEEE.&lt;/li&gt;
&lt;li&gt;Moore, R. K. (2017). Is spoken language all-or-nothing? Implications for future speech-based human-machine interaction. In Dialogues with Social Robots (pp. 281-291). Springer, Singapore.&lt;/li&gt;
&lt;li&gt;Beirl, D., Yuill, N., &amp;amp; Rogers, Y. (2019). Using Voice Assistant Skills in Family Life. In Lund, K., Niccolai, G. P., Lavoué, E., Gweon, C. H., &amp;amp; Baker, M. (Eds.), A Wide Lens: Combining Embodied, Enactive, Extended, and Embedded Learning in Collaborative Settings, 13th International Conference on Computer Supported Collaborative Learning (CSCL) 2019, Volume 1 (pp. 96-103). Lyon, France: International Society of the Learning Sciences.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
