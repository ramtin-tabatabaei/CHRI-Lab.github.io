<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>voice-assistant | computational human-robot interaction lab</title>
    <link>https://CHRI-Lab.github.io/tag/voice-assistant/</link>
      <atom:link href="https://CHRI-Lab.github.io/tag/voice-assistant/index.xml" rel="self" type="application/rss+xml" />
    <description>voice-assistant</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Research Group in HRI at CSE UNSW</copyright>
    <image>
      <url>https://CHRI-Lab.github.io/images/icon_hu02c0217b7eeb0e7fdb8b09af6bff09b6_8240_512x512_fill_lanczos_center_2.png</url>
      <title>voice-assistant</title>
      <link>https://CHRI-Lab.github.io/tag/voice-assistant/</link>
    </image>
    
    <item>
      <title>Find Your Voice: Use of Voice Assistant for Learning</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/fyv_2020/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/fyv_2020/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://CHRI-Lab.github.io/img/student-projects/fyv2020.png&#34; alt=&#34;FYV&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Many children struggle to find their voice in social situations. They may be shy, suffer from social anxiety, be new to a culture (migrants) or have impairments in communication due to atypical development  (e.g., autism spectrum disorder, speech or hearing disorders).&lt;/p&gt;
&lt;p&gt;The voices of these children often go unheard, as they find it hard to contribute to a conversation.&lt;/p&gt;
&lt;p&gt;The Find your Voice (FyV, &lt;a href=&#34;http://wafa.johal.org/project/fyv/&#34;&gt;http://wafa.johal.org/project/fyv/&lt;/a&gt;) project was initiated to investigate how joke telling could help children to speak up and gain confidence. We are also interested in story telling and general conversation. Improvements in communication can have a significant impact in confidence, and  help children:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reduce stress&lt;/li&gt;
&lt;li&gt;improve self-confidence&lt;/li&gt;
&lt;li&gt;ease social interactions&lt;/li&gt;
&lt;li&gt;make friends more easily&lt;/li&gt;
&lt;li&gt;improving literacy and language&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To help children develop the ability to communicate, tell jokes or stories to their peers, we propose leveraging social robots (e.g. NAO) and voice assistants (e.g., Alexa, Olly and Google Home) to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model how to tell jokes/stories and respond to other children during conversations .&lt;/li&gt;
&lt;li&gt;Practice joke/story telling with a ‘friendly’ and ‘non-judgmental’ audience.&lt;/li&gt;
&lt;li&gt;Practice turn taking during conversation.&lt;/li&gt;
&lt;li&gt;TLearn jokes, stories and interesting facts to tell other children.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The overall goals of the project are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To enable children to improve their social communication skills by learning intonation and timing, through interacting with voice assistants&lt;/li&gt;
&lt;li&gt;To learn to how to perform in front of peers and family&lt;/li&gt;
&lt;li&gt;To make children more confident in social situations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The FyV project involves partners in London and California.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;At UNSW, our main goal will be to develop a ‘Learning by Teaching’ application using a robot or voice assistant. This application will allow the user to teach a virtual agent (robot or voice assistant) a joke/story. As the agent learns by demonstration, the user can practice and refine how the story/joke is told until the voice assistant (and the child) is able to tell the joke/story in a satisfactory way.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Design the Learning Scenario&lt;/li&gt;
&lt;li&gt;Explore TTS software for speech conversion&lt;/li&gt;
&lt;li&gt;Implement a new Alexa Skill&lt;/li&gt;
&lt;li&gt;Run a Pilot demonstrating the learning of joke/story telling features (e.g. pauses and intonations)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Voice Assistant, Machine Learning, HCI&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python or C++. Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/soobinseo/Transformer-TTS&#34;&gt;https://github.com/soobinseo/Transformer-TTS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/barronalex/Tacotron&#34;&gt;https://github.com/barronalex/Tacotron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.amazon.com/en-US/alexa/alexa-skills-kit&#34;&gt;https://developer.amazon.com/en-US/alexa/alexa-skills-kit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Voice for ROS</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/voice-robot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/voice-robot/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Natural language is an important part of communication since it offers an intuitive and efficient way of conveying ideas to another individual. Enabling robots to efficiently use language is essential for human-robot collaboration. In this project, we aim to develop an interface between a dialog manager (i.e. DialogFlow) and ROS (Robotics Operating System). By doing this, we will be able to use the powerful dialogue systems in human-robot interaction scenario.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1066/1*OCuPx7AmWofWptQdPN-TPA.png&#34; alt=&#34;robot-va&#34;&gt;&lt;/p&gt;
&lt;p&gt;A scenario, using tangible robots (Cellulo) combined with voice assistant for upper-arm rehabilitation will be implemented to show the potential of this new ros-package.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;p&gt;During this project, the student will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn about Google DialogFlow and ROS&lt;/li&gt;
&lt;li&gt;Develop a ROS package that enables to access and manipulates DialogFlow features&lt;/li&gt;
&lt;li&gt;Develop a Cellulo Rehabilitation Game&lt;/li&gt;
&lt;li&gt;Test the game with a pilot experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Voice-Assistant, Human-Robot Interaction, ROS&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, C++,  ROS, Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://dialogflow.com/&#34;&gt;https://dialogflow.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ros.org/&#34;&gt;https://www.ros.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wafa.johal.org/project/cellulo/&#34;&gt;http://wafa.johal.org/project/cellulo/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hudson, C., Bethel, C. L., Carruth, D. W., Pleva, M., Juhar, J., &amp;amp; Ondas, S. (2017, October). A training tool for speech driven human-robot interaction applications. In 2017 15th International Conference on Emerging eLearning Technologies and Applications (ICETA) (pp. 1-6). IEEE.&lt;/li&gt;
&lt;li&gt;Moore, R. K. (2017). Is spoken language all-or-nothing? Implications for future speech-based human-machine interaction. In Dialogues with Social Robots (pp. 281-291). Springer, Singapore.&lt;/li&gt;
&lt;li&gt;Beirl, D., Yuill, N., &amp;amp; Rogers, Y. (2019). Using Voice Assistant Skills in Family Life. In Lund, K., Niccolai, G. P., Lavoué, E., Gweon, C. H., &amp;amp; Baker, M. (Eds.), A Wide Lens: Combining Embodied, Enactive, Extended, and Embedded Learning in Collaborative Settings, 13th International Conference on Computer Supported Collaborative Learning (CSCL) 2019, Volume 1 (pp. 96-103). Lyon, France: International Society of the Learning Sciences.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
